# -*- coding: utf-8 -*-
"""Dependency_Parsing_ZSTsa_hi_only_engdep_GNN25GB RAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KnhiOCOTMVlf-4cWflCsG1PDbwUL4qHc

# New Section
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install conllu

import conllu
import unicodedata, re
from collections import deque
import tensorflow as tf
from tensorflow import keras

import numpy as np
import pandas as pd
import re
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

import nltk
nltk.download('punkt')

def preorderTraversal(root):
    word_order = {} # as we don't get the original sentence token order using parsetree
    Stack = deque([])
    # 'Preorder'-> contains all the
    # visited nodes.
    Preorder =[]
    Preorder.append(root.token['form'])
    word_order[root.token['form']] = root.token['id']
    Stack.append(root)
    while len(Stack)>0:
        flag = 0
        if len((Stack[len(Stack)-1]).children)== 0:
            X = Stack.pop()
        else:
            Par = Stack[len(Stack)-1]
        for i in range(0, len(Par.children)):
            if Par.children[i].token['form'] not in Preorder:
                flag = 1
                Stack.append(Par.children[i])
                Preorder.append(Par.children[i].token['form'])
                word_order[Par.children[i].token['form']] = Par.children[i].token['id']
                break;
        if flag == 0:
            Stack.pop()
    # print(Preorder)
    return Preorder, word_order

def postorderTraversal(root):
    word_order = {} # as we don't get the original sentence token order using parsetree
    Postorder =[]
    if not root:
        return []
    Stack = [root]
    last = None

    while Stack:
        root = Stack[-1]
        if not root.children or last and (last in root.children):
            Postorder.append(root.token['form'])
            word_order[root.token['form']] = root.token['id']
            Stack.pop()
            last = root
        else:
            for children in root.children[::-1]:
                Stack.append(children)
    # print(Postorder)
    return Postorder, word_order

global_word2token = {}
global_edge_list = []

global_word2token['<pad>'] = 0
global_id_counter = 1

def DFSUtil(v, visited):
    global global_id_counter
    visited.add(v.token['id'])
    if v.token['form'] in global_word2token:
        global_id = global_word2token[v.token['form']]
    else:
        global_id = global_id_counter
        global_id_counter += 1
        global_word2token[v.token['form']] = global_id
    print(v.token['form'], end=' ')

    for neighbour in v.children:
        if neighbour.token['id'] not in visited:
            if neighbour.token['form'] in global_word2token:
                global_id_neighbour = global_word2token[neighbour.token['form']]
            else:
                global_id_neighbour = global_id_counter
                global_id_counter += 1
                global_word2token[neighbour.token['form']] = global_id_neighbour
            global_edge_list.append([global_id, global_id_neighbour])
            DFSUtil(neighbour, visited)

def DFS(v):
    visited = set()
    DFSUtil(v, visited)

!pip install indic-nlp-library

!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git

import sys
from indicnlp import common

# The path to the local git repo for Indic NLP library
INDIC_NLP_LIB_HOME=r"indic_nlp_library"

# The path to the local git repo for Indic NLP Resources
INDIC_NLP_RESOURCES=r"indic_nlp_resources"

# Add library to Python path
sys.path.append(r'{}\src'.format(INDIC_NLP_LIB_HOME))

# Set environment variable for resources folder
common.set_resources_path(INDIC_NLP_RESOURCES)

!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

#for target test file (hindi test file)
import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/hin_test_new1.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
hindilines=hinditext_file.readlines()
print(len(hindilines))
tokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=188
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
for i in range(len(hindilines)):
  hindilines1="sos " + hindilines[i].strip("\n")+ " eos"
  #print(hindilines1)
  tokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    tokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
hin_test=tokens
print(hin_test)
#print(hin_sent_tok)

line=0
line_lsth=[]
hinlst=[]
for i in hin_test:
  line=line+1
  if i not in hinlst:
    line_lsth.append(line)
    hinlst.append(i)

hinlst[4]

hinlst.pop(4)

hinlst[95]

hinlst.pop(95)

#for source test file (sankrit test file)
import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/san_test1_new1.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
sanlines=hinditext_file.readlines()
tokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=188
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
for i in range(len(sanlines)):
  hindilines1=sanlines[i].strip("\n")
  #print(hindilines1)
  tokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    tokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
san_test=tokens
print(san_test)
#print(hin_sent_tok)

line=0
line_lst=[]
sanlst=[]
for i in san_test:
  line=line+1
  if i not in sanlst:
    line_lst.append(line)
    sanlst.append(i)

print(line_lsth)
print(line_lst)

hinlst[4]

sanlst[4]

for i in range(0,145):
  print(hinlst[i])
  print(sanlst[i])

#count=0
lst1=[]
lst2=[]
lst3=[]
lst4=[]
lst5=[]
lst6=[]
lst7=[]
lst8=[]
for line in sanlst:
  print(line)
  count=0
  for word in line:
    count=count+1
  print(count)
  if count <=10:
    lst1.append(line)
  elif count>10 and count<=20:
    lst2.append(line)
  elif count>20 and count<=30:
    lst3.append(line)
  elif count>30 and count<=40:
    lst4.append(line)
  elif count>40 and count<=50:
    lst5.append(line)
  elif count>50 and count<=60:
    lst6.append(line)
  elif count>60 and count<=70:
    lst7.append(line)
  elif count>70 and count<=80:
    lst8.append(line)

print(hin_test[92])
print(san_test[92])

num_lines=10000

import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/hin_efiltered1_new3.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
hindilines=hinditext_file.readlines()
hin_etokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=num_lines
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
j=0
k=5000
for i in range(j,k):

#for i in range(len(hindilines)):
  hindilines1="sos " + hindilines[i].strip("\n")+ " eos"
  #print(hindilines1)
  hin_etokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    hin_etokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
#hin_etokens=tokens
print(hin_etokens)
print(len(hindilines))

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
eng_sent_tokens=[]
eng_stokens=[]
engtext_file = open("drive/MyDrive/eng_sfiltered3.txt", "r", encoding="utf-8")
#engtext_file = open("drive/MyDrive/itihasa-main/data/train.en", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1="sos " + englines[i] + " eos"
  eng_stokens.append([])
  for t in word_tokenize(englines1):
    eng_stokens[i].append(t)
  eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(eng_stokens)
#print(eng_sent_tokens)

print(len(eng_stokens))

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
#eng_sent_tokens=[]
san_etokens=[]
#engtext_file = open("drive/MyDrive/itihasa-main/data/train.sn", "r", encoding="utf-8")
engtext_file = open("drive/MyDrive/san_efiltered3_tok.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1= "en "+ englines[i]
  san_etokens.append([])
  for t in word_tokenize(englines1):
    san_etokens[i].append(t)
  eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(san_etokens)
print(len(englines))

print(san_etokens[1])

print(len(san_etokens))

print(san_etokens[0])

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
#eng_sent_tokens=[]
eng_htokens=[]
engtext_file = open("drive/MyDrive/eng_hfiltered1_tok_new3.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1= englines[i]
  eng_htokens.append([])
  for t in word_tokenize(englines1):
    eng_htokens[i].append(t + ' ')
  #eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(eng_htokens)
print(len(englines))

target_train_tokens=[]
for i in eng_stokens:
  target_train_tokens.append(i)

for i in hin_etokens:
  target_train_tokens.append(i)

#for i in hin_test:
target_tokens=[]
for i in hinlst:
  target_tokens.append(i)

#target_tokens=[]
for i in hin_etokens:
  target_tokens.append(i)
print(len(hin_etokens))

for i in eng_stokens:
  target_tokens.append(i)
print(len(eng_sent_tokens))

data_file = open("drive/MyDrive/san_test.conllu", "r", encoding="utf-8")
all_corpus_text_test_s=[]
all_corpus_index_test_order_s=[]
all_corpus_text_test_s_gnn = []
reduce_count=148

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
    DFS(tokentree)
  except Exception:
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]
  node_indices = [global_word2token[w] for w in normal_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order, "\nGNN involved node indices: ", node_indices)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-orderindex: ", pre_order_index_input, "\nPost-orderindex: ", post_order_index_input)

  all_corpus_text_test_s.append([normal_order,pre_order,post_order])
  all_corpus_text_test_s_gnn.append(node_indices)
  all_corpus_index_test_order_s.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

####skip san dep parsing####

###save normal,pre,post as san sentence)
all_corpus_text_test_s=[]
#for i in range(len(san_test)):
for i in range(len(sanlst)):
  #all_corpus_text_test_s.append([san_test[i],san_test[i],san_test[i]])
  all_corpus_text_test_s.append([sanlst[i],sanlst[i],sanlst[i]])
#### storing index###Pre Post###

z=0
p=0
index0=[]
#for i in san_test:
for i in sanlst:
  index0.append([])
  for word in i:
    index=z
    index0[p].append(index)
    z=z+1
    if z==len(i):
      z=0
  p=p+1
index1=[]
for i in index0:
  array=np.array(i)
  index1.append(array)
all_corpus_index_test_order_s=[]
for i in index1:
  all_corpus_index_test_order_s.append([i,i])

#####storing index san text end ####

data_file = open("drive/MyDrive/san_e.conllu", "r", encoding="utf-8")
all_corpus_text_s=[]
all_corpus_text_s_gnn=[]
all_corpus_index_order_s=[]
reduce_count=5000

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
    DFS(tokentree)
  except Exception:
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]
  node_indices = [global_word2token[w] for w in normal_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order, "\nGNN involved node indices: ", node_indices)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-orderindex: ", pre_order_index_input, "\nPost-orderindex: ", post_order_index_input)

  all_corpus_text_s.append([normal_order,pre_order,post_order,node_indices])
  all_corpus_text_s_gnn.append(node_indices)
  all_corpus_index_order_s.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

#####storing san train data and indices for pre post begin###

####skip san dep parsing train san sentences####
#### storing index###Pre Post###
all_corpus_text_s=[]
for i in range(len(san_etokens)):
  all_corpus_text_s.append([san_etokens[i],san_etokens[i],san_etokens[i]])
z=0
p=0
index0=[]
for i in san_etokens:

  index0.append([])
  for word in i:
    index=z
    index0[p].append(index)
    z=z+1
    if z==len(i):
      z=0
  p=p+1
#all_corpus_index_test_order_s1=np.array
index1=[]
for i in index0:
  array=np.array(i)
  index1.append(array)
all_corpus_index_order_s=[]
for i in index1:
  all_corpus_index_order_s.append([i,i])

###end san train skip dep####

print(len(all_corpus_text_s))

print(all_corpus_index_order_s[0])

data_file = open("drive/MyDrive/eng_h.conllu", "r", encoding="utf-8")
all_corpus_text_e=[]
all_corpus_text_e_gnn=[]
all_corpus_index_order_e=[]
reduce_count=5000

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
    DFS(tokentree)
  except Exception:
    # all_corpus_text_e.append([None,None,None])
    # all_corpus_index_order_e.append([None,None])
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]
  node_indices = [global_word2token[w] for w in normal_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order, "\nGNN involved node indices: ", node_indices)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-order-index: ", pre_order_index_input, "\nPost-order-index: ", post_order_index_input)

  all_corpus_text_e.append([normal_order,pre_order,post_order])
  all_corpus_text_e_gnn.append(node_indices)
  all_corpus_index_order_e.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

# data_file = open("drive/MyDrive/eng_h.conllu", "r", encoding="utf-8")
# all_corpus_text_e=[]
# all_corpus_index_order_e=[]
# reduce_count=5000

# for tokentree in conllu.parse_tree_incr(data_file):
#   print(tokentree)
#   print(tokentree.print_tree())
#   print(tokentree.metadata['text'])
#   try:
#     pre_order, normal_order  = preorderTraversal(tokentree)
#   except Exception:
#     all_corpus_text.append([None,None,None])
#     all_corpus_index_order.append([None,None])
#     continue
#   post_order, _ = postorderTraversal(tokentree)
#   normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
#   normal_order = [w.lower() for w in normal_order]
#   pre_order = [w.lower() for w in pre_order]
#   post_order = [w.lower() for w in post_order]

#   print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order)

#   pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
#   post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
#   print("\nPre-order-index: ", pre_order_index_input, "\nPost-order-index: ", post_order_index_input)

#   all_corpus_text_e.append([normal_order,pre_order,post_order])
#   all_corpus_index_order_e.append([pre_order_index_input,post_order_index_input])

#   reduce_count-=1
#   if reduce_count<=0:
#     break

all_corpus_text_source=[]
for i in all_corpus_text_e:
  all_corpus_text_source.append(i)
print(len(all_corpus_text_e))

for i in all_corpus_text_s:
  all_corpus_text_source.append(i)
print(len(all_corpus_text_s))# getting more sentences from san_e conllu##check later

len(eng_htokens)

source_tokens=[]
for i in all_corpus_text_test_s:
  source_tokens.append(i)
print(len(all_corpus_text_test_s))

for i in all_corpus_text_e:
  source_tokens.append(i)

for i in all_corpus_text_s:
  source_tokens.append(i)

all_corpus_text_source_gnn=[]
for i in all_corpus_text_test_s_gnn:
  all_corpus_text_source_gnn.append(i)

for i in all_corpus_text_e_gnn:
  all_corpus_text_source_gnn.append(i)

for i in all_corpus_text_s_gnn:
  all_corpus_text_source_gnn.append(i)

print(source_tokens)

all_corpus_index_order=[]
for i in all_corpus_index_test_order_s:
  all_corpus_index_order.append(i)

for i in all_corpus_index_order_e:
  all_corpus_index_order.append(i)

for i in all_corpus_index_order_s:
  all_corpus_index_order.append(i)

len(all_corpus_text_source), len(all_corpus_index_order)

len(all_corpus_text_source), len(all_corpus_index_order)

#import itertools
import more_itertools as mit


vocab_tar=set(mit.collapse(target_tokens))
#print(flat_list_h)
# {'are', 'good', 'hello', 'hey', 'hi', 'how', 'jane', 'morning', 'top', 'you'}
# flat_list_h = list(itertools.chain(hindi_tokens))
# flat_list_h = list(itertools.chain(flat_list_h))
# vocab_h = sorted(set(flat_list_h))
print(len(vocab_tar))
word2ind_tar = {}
ind2word_tar = {}

ind=1
for word in vocab_tar:
    word2ind_tar[word] = ind
    ind+=1
ind2word_tar = {v: k for k, v in word2ind_tar.items()}

import itertools

flat_list_e = list(itertools.chain(*source_tokens))
flat_list_e = list(itertools.chain(*flat_list_e))
vocab_source = sorted(set(flat_list_e))
print(len(flat_list_e), len(vocab_source))
word2ind_source = {}
ind2word_source = {}

ind=1
for word in vocab_source:
    word2ind_source[word] = ind
    ind+=1
ind2word_source = {v: k for k, v in word2ind_source.items()}

print(vocab_source)

vocab_size_target = len(vocab_tar)+1
constant_batch_size = 128

vocab_size_source = len(vocab_source)+1
constant_batch_size = 128

#corpus_tr = [[word2ind_tar[word] for word in sample] for sample in target_train_tokens ]#for order in sample]#target->hindi

corpus_tr = [[word2ind_tar[word] for word in sample] for sample in target_tokens ]#for order in sample]#target->hindi

#corpus_tr_test = [[word2ind_tar[word] for word in sample] for sample in hin_test ]#for order in sample]#target->hindi

corpus_tr_test = [[word2ind_tar[word] for word in sample] for sample in hinlst]#for order in sample]#target->hindi

#corpus_sr = [[word2ind_source[word] for word in order] for sample in all_corpus_text_source for order in sample]#source->english

corpus_sr = [[word2ind_source[word] for word in order] for sample in source_tokens for order in sample]#source->english

#corpus_sr_test = [[word2ind_source[word] for word in order] for sample in all_corpus_text_test_s for order in sample]#source->english

corpus_sr_test = [[word2ind_source[word] for word in order] for sample in all_corpus_text_test_s for order in sample]#source->english

print(all_corpus_text_test_s)

max_length_tr_train = max([len(s) for s in corpus_tr])
print(max_length_tr_train)

z=0
line_numb=[]
for s in hin_test:
  z=z+1
  if len(s)>80:
   line_numb.append(z)
   print(s)
  #  for i in s:
  #    print(ind2word_tar[i])
print(line_numb)

max_length_tr_test = max([len(s) for s in corpus_tr_test])
print(max_length_tr_test)

max_length_tr=max(max_length_tr_train,max_length_tr_test)
print(max_length_tr)

max_length_tr_test = max([len(s) for s in corpus_tr_test])
print(max_length_tr_test)

max_length_sr_train = max([len(s) for s in corpus_sr])
print(max_length_sr_train)

max_length_sr_test = max([len(s) for s in corpus_sr_test])
print(max_length_sr_test)

max_length_sr = max(max_length_sr_train,max_length_sr_test)
print(max_length_sr)

max_length = max(max_length_tr,max_length_sr)

max_length_tr = max_length
max_length_sr = max_length

corpus_pad_tr = keras.preprocessing.sequence.pad_sequences(corpus_tr,maxlen=max_length_tr,padding='post')

corpus_pad_tr_test = keras.preprocessing.sequence.pad_sequences(corpus_tr_test,maxlen=max_length_tr,padding='post')

corpus_pad_sr = keras.preprocessing.sequence.pad_sequences(corpus_sr,maxlen=max_length_sr,padding='post')

corpus_pad_sr_test = keras.preprocessing.sequence.pad_sequences(corpus_sr_test,maxlen=max_length_sr,padding='post')

corpus_pad_tr = corpus_pad_tr.reshape(-1,max_length_tr)
corpus_pad_tr.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_tr_test = corpus_pad_tr_test.reshape(-1,max_length_tr)
corpus_pad_tr_test.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_sr = corpus_pad_sr.reshape(-1,3,max_length_sr)
corpus_pad_sr.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_sr_test = corpus_pad_sr_test.reshape(-1,3,max_length_sr)
corpus_pad_sr_test.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

y=[0]*10 + [1]*10

print(corpus_pad_tr[0])

y

all_co

# to include zero padding indices to corpus index too

all_corpus_index_order_1D= [order for sample in all_corpus_index_order for order in sample]

all_corpus_index_order_1D_test= [order for sample in all_corpus_index_test_order_s for order in sample]

all_corpus_index_order_1D_pad=[]
for sent in all_corpus_index_order_1D:
    sent_len = len(sent)
    print(sent_len)
    if sent_len < max_length_sr: # logic for padding the index
        new_sent = list(sent)+list(range(sent_len,max_length_sr))
    all_corpus_index_order_1D_pad.append(np.array(new_sent))
all_corpus_index_order_1D_pad = np.array(all_corpus_index_order_1D_pad)

all_corpus_index_order_1D_pad_test=[]
for sent in all_corpus_index_order_1D_test:
    sent_len = len(sent)
    print(sent_len)
    if sent_len < max_length_sr: # logic for padding the index
        new_sent = list(sent)+list(range(sent_len,max_length_sr))
    all_corpus_index_order_1D_pad_test.append(np.array(new_sent))
all_corpus_index_order_1D_pad_test = np.array(all_corpus_index_order_1D_pad_test)

len(all_corpus_index_order_1D_pad_test)

for sent in all_corpus_index_order_1D_pad:
    sent_len = len(sent)
    print(sent_len)

for sent in all_corpus_index_order_1D_pad_test:
    sent_len = len(sent)
    print(sent_len)

all_corpus_index_order_1D_pad_reshape = all_corpus_index_order_1D_pad.reshape(-1,2,all_corpus_index_order_1D_pad.shape[-1])

all_corpus_index_order_1D_pad_reshape_test = all_corpus_index_order_1D_pad_test.reshape(-1,2,all_corpus_index_order_1D_pad_test.shape[-1])

all_corpus_index_order_1D_pad_reshape.shape, corpus_pad_sr.shape

all_corpus_index_order_1D_pad_reshape_test.shape, corpus_pad_sr_test.shape

#for english#source language
input_set_X1,input_set_X2,input_set_X3,input_set_X4,input_set_X5,input_set_X6=[],[],[],[],[],[]
#for sample_ind in range(len(corpus_pad_sr)):
for sample_ind in range(0,1280):
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840):
#for sample_ind in range(0,67):
#for sample_ind in range(5120,6400):
#for sample_ind in range(6400,7680):
#for sample_ind in range(8868,10148):

#for sample_ind in range(len(X_train)):#for different test data set
    preorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape[sample_ind][0].reshape(corpus_pad_sr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    postorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape[sample_ind][1].reshape(corpus_pad_sr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1.append(corpus_pad_sr[sample_ind][0])
    input_set_X2.append(corpus_pad_sr[sample_ind][1])
    input_set_X3.append(corpus_pad_sr[sample_ind][2])
    input_set_X4.append(preorder_sampled)#index_preorder
    input_set_X5.append(postorder_sampled)#index_postorder
    input_set_X6.append(all_corpus_text_source_gnn[sample_ind]) # GNN indices

input_set_X1_test,input_set_X2_test,input_set_X3_test,input_set_X4_test,input_set_X5_test,input_set_X6_test=[],[],[],[],[],[]
for sample_ind in range(128):
#for sample_ind in range(0,67):
#for sample_ind in range(len(X_train)):#for different test data set
    preorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape_test[sample_ind][0].reshape(corpus_pad_sr_test.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    postorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape_test[sample_ind][1].reshape(corpus_pad_sr_test.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_test.append(corpus_pad_sr_test[sample_ind][0])
    input_set_X2_test.append(corpus_pad_sr_test[sample_ind][1])
    input_set_X3_test.append(corpus_pad_sr_test[sample_ind][2])
    input_set_X4_test.append(preorder_sampled)#index_preorder
    input_set_X5_test.append(postorder_sampled)#index_postorder
    input_set_X6_test.append(all_corpus_text_test_s_gnn[sample_ind])

#for hindi#target language
input_set_X1_target=[]
#for sample_ind in range(len(corpus_pad_tr)):#len(corpus_pad_tr)
for sample_ind in range(0,1280):#len(corpus_pad_tr)
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840):
#for sample_ind in range(5120,6400):
#for sample_ind in range(6400,7680):
#for sample_ind in range(8868,10148):
#for sample_ind in range(len(Y_train)):#len(corpus_pad_tr)for different test data set
    # preorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][0].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    # postorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][1].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_target.append(corpus_pad_tr[sample_ind])
    # input_set_X2_h.append(corpus_pad_tr[sample_ind][1])
    # input_set_X3_h.append(corpus_pad_tr[sample_ind][2])
    # input_set_X4_h.append(preorder_sampled_h)#index_preorder
    # input_set_X5_h.append(postorder_sampled_h)#index_postorder

    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

#for hindi#target language
input_set_X1_target_test=[]
#for sample_ind in range(len(corpus_pad_tr_test)):#len(corpus_pad_tr)
for sample_ind in range(0,128):#len(corpus_pad_tr)

#for sample_ind in range(len(Y_train)):#len(corpus_pad_tr)for different test data set
    # preorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][0].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    # postorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][1].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_target_test.append(corpus_pad_tr_test[sample_ind])
    # input_set_X2_h.append(corpus_pad_tr[sample_ind][1])
    # input_set_X3_h.append(corpus_pad_tr[sample_ind][2])
    # input_set_X4_h.append(preorder_sampled_h)#index_preorder
    # input_set_X5_h.append(postorder_sampled_h)#index_postorder

    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

pwd

import pickle
embeddings_index = {}  # Create an empty dictionary
with open('drive/MyDrive/data_en_source_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file1 to the dictionary

with open('drive/MyDrive/data_en_target_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file1 to the dictionary

with open('drive/MyDrive/data_sa_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file2 to the dictionary

with open('drive/MyDrive/data_hi_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file2 to the dictionary
#print (my_dict_final)



embedding_matrix_source = np.zeros((len(word2ind_source) + 1, 300))
for word, i in word2ind_source.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_source[i] = embedding_vector

word2ind_tar

for word, i in word2ind_tar.items():
    #print(word,i)
    embedding_vector = embeddings_index.get(word)
    print(word,embedding_vector)

ind2word_tar[14575]

embedding_matrix_target = np.zeros((len(word2ind_tar) + 1, 300))
z=0
for word, i in word2ind_tar.items():
    #print(word,i)
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_target[i] = embedding_vector
        #print(i,embedding_matrix_target)
#     else:
#     #     embedding_matrix_target[i]="OOV"
#         z=z+1
#         #print(word,i)
# print(z)

global_word2token = dict(sorted(global_word2token.items(), key=lambda x: x[1]))

global_token2word = {v: k for k, v in global_word2token.items()}
global_token2word = dict(sorted(global_token2word.items(), key=lambda x: x[0]))

hit = 0
node_feature_matrix = np.random.rand(len(global_word2token), 300)
for word, i in global_word2token.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        hit += 1
        node_feature_matrix[i] = embedding_vector

print(hit, len(global_word2token), hit*100/len(global_word2token))

#from keras.utils import to_categorical
from tensorflow.keras.utils import to_categorical


X1_h_new_list=to_categorical(input_set_X1_target, num_classes = vocab_size_target, dtype ="int32")

from tensorflow.keras.layers import Embedding, LSTM, Dense, Concatenate, Lambda,Bidirectional,Attention
from keras.layers import  RepeatVector

from tensorflow.keras import Model, Input
from tensorflow.keras.utils import plot_model
#from keras.layers.wrappers import TimeDistributed
from tensorflow.keras import layers

import pathlib
import random
import string
import re
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import TextVectorization

class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(dense_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype="int32")
        attention_output = self.attention(
            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask
        )
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "dense_dim": self.dense_dim,
            "num_heads": self.num_heads,
            "attention": self.attention,
            "dense_proj": self.dense_proj,
            "layernorm_1": self.layernorm_1,
            "layernorm_2": self.layernorm_2,
            "supports_masking": self.supports_masking,

        })
        return config


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)

    def get_config(self):
        config = super().get_config()
        config.update({
            "token_embeddings": self.token_embeddings,
            "position_embeddings": self.position_embeddings,
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim,
        })
        return config


class TransformerDecoder(layers.Layer):
    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.latent_dim = latent_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(latent_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
            padding_mask = tf.minimum(padding_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        proj_output = self.dense_proj(out_2)
        return self.layernorm_3(out_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "latent_dim": self.latent_dim,
            "num_heads": self.num_heads,
            "attention_1": self.attention_1,
            "attention_2": self.attention_2,
            "dense_proj": self.dense_proj,
            "layernorm_1": self.layernorm_1,
            "layernorm_2": self.layernorm_2,
            "layernorm_3": self.layernorm_3,
            "supports_masking": self.supports_masking,

        })
        return config

def create_ffn(hidden_units, dropout_rate, name=None):
    fnn_layers = []

    for units in hidden_units:
        fnn_layers.append(layers.BatchNormalization())
        fnn_layers.append(layers.Dropout(dropout_rate))
        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))

    return keras.Sequential(fnn_layers, name=name)

# https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/graph/ipynb/gnn_citations.ipynb#scrollTo=rZLBi_8Ot3SH

class GraphConvLayer(layers.Layer):
    def __init__(
        self,
        hidden_units,
        dropout_rate=0.2,
        aggregation_type="mean",
        combination_type="concat",
        normalize=False,
        *args,
        **kwargs,
    ):
        super(GraphConvLayer, self).__init__(*args, **kwargs)

        self.aggregation_type = aggregation_type
        self.combination_type = combination_type
        self.normalize = normalize

        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)
        if self.combination_type == "gated":
            self.update_fn = layers.GRU(
                units=hidden_units,
                activation="tanh",
                recurrent_activation="sigmoid",
                dropout=dropout_rate,
                return_state=True,
                recurrent_dropout=dropout_rate,
            )
        else:
            self.update_fn = create_ffn(hidden_units, dropout_rate)

    def prepare(self, node_repesentations, weights=None):
        # node_repesentations shape is [num_edges, embedding_dim].
        messages = self.ffn_prepare(node_repesentations)
        if weights is not None:
            messages = messages * tf.expand_dims(weights, -1)
        return messages

    def aggregate(self, node_indices, neighbour_messages):
        # node_indices shape is [num_edges].
        # neighbour_messages shape: [num_edges, representation_dim].
        num_nodes = tf.math.reduce_max(node_indices) + 1
        if self.aggregation_type == "sum":
            aggregated_message = tf.math.unsorted_segment_sum(
                neighbour_messages, node_indices, num_segments=num_nodes
            )
        elif self.aggregation_type == "mean":
            aggregated_message = tf.math.unsorted_segment_mean(
                neighbour_messages, node_indices, num_segments=num_nodes
            )
        elif self.aggregation_type == "max":
            aggregated_message = tf.math.unsorted_segment_max(
                neighbour_messages, node_indices, num_segments=num_nodes
            )
        else:
            raise ValueError(f"Invalid aggregation type: {self.aggregation_type}.")

        return aggregated_message

    def update(self, node_repesentations, aggregated_messages):
        # node_repesentations shape is [num_nodes, representation_dim].
        # aggregated_messages shape is [num_nodes, representation_dim].
        if self.combination_type == "gru":
            # Create a sequence of two elements for the GRU layer.
            h = tf.stack([node_repesentations, aggregated_messages], axis=1)
        elif self.combination_type == "concat":
            # Concatenate the node_repesentations and aggregated_messages.
            h = tf.concat([node_repesentations, aggregated_messages], axis=1)
        elif self.combination_type == "add":
            # Add node_repesentations and aggregated_messages.
            h = node_repesentations + aggregated_messages
        else:
            raise ValueError(f"Invalid combination type: {self.combination_type}.")

        # Apply the processing function.
        node_embeddings = self.update_fn(h)
        if self.combination_type == "gru":
            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]

        if self.normalize:
            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)
        return node_embeddings

    def call(self, inputs):
        """Process the inputs to produce the node_embeddings.

        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.
        Returns: node_embeddings of shape [num_nodes, representation_dim].
        """

        node_repesentations, edges, edge_weights = inputs
        # Get node_indices (source) and neighbour_indices (target) from edges.
        node_indices, neighbour_indices = edges[0], edges[1]
        # neighbour_repesentations shape is [num_edges, representation_dim].
        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)

        # Prepare the messages of the neighbours.
        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)
        # Aggregate the neighbour messages.
        aggregated_messages = self.aggregate(node_indices, neighbour_messages)
        # Update the node embedding with the neighbour messages.
        return self.update(node_repesentations, aggregated_messages)

class GNNNodeFeature(tf.keras.Model):
    def __init__(
        self,
        graph_info,
        hidden_units,
        aggregation_type="sum",
        combination_type="concat",
        dropout_rate=0.2,
        normalize=True,
        *args,
        **kwargs,
    ):
        super(GNNNodeFeature, self).__init__(*args, **kwargs)

        # Unpack graph_info to three elements: node_features, edges, and edge_weight.
        node_features, edges, edge_weights = graph_info
        self.node_features = node_features
        self.edges = edges
        self.edge_weights = edge_weights
        # Set edge_weights to ones if not provided.
        if self.edge_weights is None:
            self.edge_weights = tf.ones(shape=edges.shape[1])
        # Scale edge_weights to sum to 1.
        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)

        # Create a process layer.
        self.preprocess = create_ffn(hidden_units, dropout_rate, name="preprocess")
        # Create the first GraphConv layer.
        self.conv1 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="graph_conv1",
        )
        # Create the second GraphConv layer.
        self.conv2 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="graph_conv2",
        )
        # Create a postprocess layer.
        self.postprocess = create_ffn(hidden_units, dropout_rate, name="postprocess")
        self.flatten = layers.Flatten()
        self.compute_dense = layers.Dense(units=300, name="features")

    def call(self, input_node_indices):
        # Preprocess the node_features to produce node representations.
        x = self.preprocess(self.node_features)
        # Apply the first graph conv layer.
        x1 = self.conv1((x, self.edges, self.edge_weights))
        # Skip connection.
        x = x1 + x
        # Apply the second graph conv layer.
        x2 = self.conv2((x, self.edges, self.edge_weights))
        # Skip connection.
        x = x2 + x
        # Postprocess node embedding.
        x = self.postprocess(x)
        # Fetch node embeddings for the input node_indices.
        node_embeddings = tf.gather(x, input_node_indices)
        # Compute average embeddings
        node_embeddings = self.flatten(node_embeddings)
        return self.compute_dense(node_embeddings)

# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].
edges = np.array(global_edge_list).T
# Create an edge weights array of ones.
edge_weights = tf.ones(shape=edges.shape[1])
# Create a node features array of shape [num_nodes, num_features].
node_features = tf.cast(node_feature_matrix, dtype=tf.dtypes.float32
)
# Create graph info tuple with node_features, edges, and edge_weights.
graph_info = (node_features, edges, edge_weights)

print("Edges shape:", edges.shape)
print("Nodes shape:", node_features.shape)

gnn_model = GNNNodeFeature(
    graph_info=graph_info,
    hidden_units=[32,32],
    dropout_rate=0.2,
    name="gnn_model",
)

tf.repeat([[[3, 4, 5]],[[1, 2, 10]]], repeats=[10], axis=1)

embed_dim = 300
latent_dim = 512
num_heads = 4

inp_layer_normal=Input(shape=(max_length,), batch_size=constant_batch_size, name="normal_input")
inp_layer_pre=Input(shape=(max_length,), batch_size=constant_batch_size, name="preorder_input")
inp_layer_post=Input(shape=(max_length,), batch_size=constant_batch_size, name="postorder_input")
inp_gnn_node_index=Input(shape=(max_length,), batch_size=constant_batch_size, dtype=tf.int32, name="gnn_node")
inp_index_pre=Input(shape=(max_length,2), batch_size=constant_batch_size, dtype=tf.int32, name="preorder_index")
inp_index_post=Input(shape=(max_length,2), batch_size=constant_batch_size, dtype=tf.int32, name="postorder_index")

x1 = PositionalEmbedding(max_length_sr, len(word2ind_source) + 1, embed_dim)(inp_layer_normal)
mid_layer_normal = TransformerEncoder(embed_dim, latent_dim, num_heads)(x1)

x2 = PositionalEmbedding(max_length_sr, len(word2ind_source) + 1, embed_dim)(inp_layer_pre)
mid_layer_pre = TransformerEncoder(embed_dim, latent_dim, num_heads)(x2)

x3 = PositionalEmbedding(max_length_sr, len(word2ind_source) + 1, embed_dim)(inp_layer_post)
mid_layer_post = TransformerEncoder(embed_dim, latent_dim, num_heads)(x3)

mid_layer_pre = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_preorder")([mid_layer_pre,inp_index_pre])
mid_layer_post = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_postorder")([mid_layer_post,inp_index_post])

# gnn_features = gnn_model(inp_gnn_node_index)
# gnn_features_for_max_len = tf.keras.layers.RepeatVector(max_length)(gnn_features)

concat_layer = Concatenate(axis=2)([mid_layer_normal,mid_layer_pre,mid_layer_post])#,gnn_features_for_max_len])

decoder_inputs = keras.Input(shape=(max_length,), dtype="int64", batch_size=constant_batch_size, name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(max_length, embed_dim*4), batch_size=constant_batch_size, name="decoder_state_inputs")
x = PositionalEmbedding(max_length_tr, vocab_size_target, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_target, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, concat_layer])

model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,inp_gnn_node_index,decoder_inputs], decoder_outputs)
# model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,decoder_inputs], decoder_outputs)


# # Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy')

print(model.summary())
plot_model(model, to_file='/content/drive/My Drive/Transformer_ZST_GNN/model.png')

# # Functional API
# from tensorflow.keras.layers import Embedding, LSTM, Dense, Concatenate, Lambda,Bidirectional,Attention
# from keras.layers import  RepeatVector

# from tensorflow.keras import Model, Input
# from tensorflow.keras.utils import plot_model
# from tensorflow.keras import layers
# #from keras.layers.wrappers import TimeDistributed
# from tensorflow.keras import layers
# EMBEDDING_DIM=300
# inp_layer_normal=Input(shape=(None,), batch_size=constant_batch_size, name="normal_input")
# inp_layer_pre=Input(shape=(None,), batch_size=constant_batch_size, name="preorder_input")
# inp_layer_post=Input(shape=(None,), batch_size=constant_batch_size, name="postorder_input")
# inp_index_pre=Input(shape=(None,2), batch_size=constant_batch_size, dtype=tf.int32, name="preorder_index")
# inp_index_post=Input(shape=(None,2), batch_size=constant_batch_size, dtype=tf.int32, name="postorder_index")


# mid_layer_normal = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_normal)
# mid_layer_pre = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_pre)
# mid_layer_post = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_post)

# embd_normal=mid_layer_normal(inp_layer_normal)
# embd_pre=mid_layer_pre(inp_layer_pre)
# embd_post=mid_layer_post(inp_layer_post)

# # mid_layer_normal=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_normal) # not sharing the embedding layer
# # mid_layer_pre=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_pre)
# # mid_layer_post=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_post)
# #encoder_lstm1 = LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)


# mid_layer_normal=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_normal)
# mid_layer_pre=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_pre)
# mid_layer_post=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_post)

# #mid_layer_normal=Bidirectional(LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True))(mid_layer_normal)
# #mid_layer_normal=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_normal)
# #mid_layer_normal=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_normal)
# #mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
# #mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
# #mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
# #mid_layer_post=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_post)
# #mid_layer_post=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_post)
# #mid_layer_post=LSTM(units=64,dropout=0.2,recuindrrent_dropout=0.2, return_sequences=True)(mid_layer_post)

# # mid_layer_normal = Lambda(lambda x: tf.gather_nd(x[0],x[1]))([mid_layer_normal,inp_index_order])
# mid_layer_pre = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_preorder")([mid_layer_pre,inp_index_pre])
# mid_layer_post = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_postorder")([mid_layer_post,inp_index_post])

# concat_layer = Concatenate(axis=2)([mid_layer_normal,mid_layer_pre,mid_layer_post])


# #concat_layer = Merge([mid_layer_normal,mid_layer_pre,mid_layer_post])

# # encoder_inputs = Input(shape=(None,))
# #enc_emb =  Embedding(input_dim=vocab_size_source,output_dim=50)(concat_layer)
# encoder_lstm = LSTM(units=EMBEDDING_DIM,return_state=True)
# encoder_outputs, state_h, state_c = encoder_lstm(concat_layer)
# # # We discard `encoder_outputs` and only keep the states.
# encoder_states = [state_h, state_c]

# #output, state_h, state_c =Concatenate(axis=2)([mid_layer_normal,mid_layer_pre,mid_layer_post])
# print(concat_layer.shape)
# print(concat_layer)

# # encoder_inputs = Input(shape=(None, num_encoder_tokens))
# # encoder = LSTM(units=64, return_state=True)
# # enc_output, state_h, state_c = LSTM(units=64, return_state=True, name="encoder")(concat_layer)
# # # encoder_outputs, state_h, state_c = concat_layer
# # # # We discard `encoder_outputs` and only keep the states.
# # encoder_states = [state_h, state_c]
# #  #Set up the decoder, using `encoder_states` as initial state.
# decoder_inputs = Input(shape=(None,), batch_size=constant_batch_size, name="decoder input")
# dec_emb_layer= Embedding(len(word2ind_tar) + 1,EMBEDDING_DIM,weights=[embedding_matrix_target],input_length=max_length_tr,trainable=True)

# #dec_emb_layer = Embedding(input_dim=vocab_size_target,output_dim=50)
# # dec_emb = dec_emb_layer(decoder_inputs)

# # # We set up our decoder to return full output sequences,
# # # and to return internal states as well. We don't use the
# # # return states in the training model, but we will use them in inference.
# decoder_embedding=dec_emb_layer(decoder_inputs) # not sharing the embedding layer
# decoder_lstm = LSTM(EMBEDDING_DIM, return_sequences=True, return_state=True)

# #decoder_lstm = LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True,return_state=True)#(decoder_embedding,initial_state=encoder_states)
# #decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
# # attn_layer = Attention(name='attention_layer')
# # attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])

# # decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

# #dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')
# # decoder_pred = dense_time(decoder_concat_input)



# decoder_dense = Dense(units=vocab_size_target, activation='softmax', name='softmax_layer')
# dense_time = tf.keras.layers.TimeDistributed(decoder_dense, name='time_distributed_layer')

# decoder_outputs = dense_time(decoder_outputs)

# # # Define the model that will turn
# # # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
# model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,decoder_inputs], decoder_outputs)

# # # Compile the model
# model.compile(optimizer='adam', loss='categorical_crossentropy')

# #output = TimeDistributed(Dense(units=vocab_size_target, activation='sigmoid'))(decoder_outputs)
# #Dense(units=1, activation='sigmoid')
# # # # output_layer = layers.Dense(10)(decoder_output)

# # # #output_layer=(Dense(units=1,activation='softmax'))(concat_layer) # To be replaced with decoder
# # # #model = keras.Model([encoder_input, decoder_input], output)

# # #model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post], output_layer)
# # # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# #model = keras.Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,decoder_inputs], output)
# #model.compile(optimizer='adam', loss='binary_crossentropy')

# #model.summary()
# print(model.summary())
# plot_model(model, to_file='/content/drive/My Drive/model.png')

def shift(lst,vocab_size_h):
    new_list = []
    for sent in lst:
      sent_list=[]
      for p in range(len(sent)-1):

        sent_list.insert(p, sent[p+1])
      z=np.zeros((vocab_size_h))
      z[0]=1
      sent_list.append(z)
      new_list.append(sent_list)
      #print("length",len(sent_list))
      #print(sent[p+1].shape)



    return new_list

out_shift=shift(X1_h_new_list,vocab_size_target)

pwd

from keras.models import  load_model

filepath = "drive/MyDrive/Transformer_ZST_GNN/model.h5"

model = load_model(filepath)

from numpy.testing import assert_allclose
from keras.models import  load_model
#from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import ModelCheckpoint,EarlyStopping

es = EarlyStopping(monitor='val_loss', mode='min', patience=5)
filepath = "drive/MyDrive/Transformer_ZST_GNN/model.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint,es]
#callbacks_list = [checkpoint]

from numpy.testing import assert_allclose
from keras.models import  load_model
from keras.callbacks import ModelCheckpoint,EarlyStopping

history=model.fit([np.array(input_set_X1),np.array(input_set_X2),np.array(input_set_X3),np.array(input_set_X4),np.array(input_set_X5),np.array(input_set_X6),np.array(input_set_X1_target)],np.array(out_shift),batch_size=constant_batch_size,epochs=2, verbose=1,callbacks=callbacks_list,validation_split=0.2)

# from numpy.testing import assert_allclose
# from keras.models import  load_model
# #from keras.layers import LSTM, Dropout, Dense
# from keras.callbacks import ModelCheckpoint,EarlyStopping

# # es = EarlyStopping(monitor='val_loss', mode='min', patience=50)
# # filepath = "drive/MyDrive/10000epocs/model.h5"
# # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
# # callbacks_list = [checkpoint,es]
# #history=model.fit([np.array(X1_train),np.array(X2_train),np.array(X3_train),np.array(X4_train),np.array(X5_train),np.array(Y_train)],np.array(out_shift_t),batch_size=constant_batch_size,epochs=10, verbose=1,callbacks=callbacks_list,validation_data=([np.array(X1_val),np.array(X2_val),np.array(X3_val),np.array(X4_val),np.array(X5_val),np.array(Y_val)],),validation_batch_size=constant_batch_size)

# history=model.fit([np.array(input_set_X1),np.array(input_set_X2),np.array(input_set_X3),np.array(input_set_X4),np.array(input_set_X5),np.array(input_set_X1_target)],np.array(out_shift),batch_size=constant_batch_size,epochs=80, verbose=1,callbacks=callbacks_list,validation_split=0.2)
# #!mkdir -p saved_model
# #model.save('saved_model/my_model')
# #checkpoint = ModelCheckpoint(filepath='/content/check_pt/model.h5', monitor='val_loss',verbose=1, save_best_only=True, mode='min')

filepath = "drive/MyDrive/dep_en_zst_25gb/model.h5"

model = load_model(filepath)

# encoder_inputs1,encoder_inputs2,encoder_inputs3,encoder_inputs4,encoder_inputs5 = model.input[0],model.input[1],model.input[2],model.input[3],model.input[4]  # input_1
# encoder_outputs, state_h_enc, state_c_enc = model.layers[16].output  # lstm_1
# encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post], encoder_states)

# Decoder setup
# Below tensors will hold the states of the previous time step
latent_dim=300
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence

# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary

# Final decoder model
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2)

print()

plot_model(decoder_model, to_file='/content/drive/My Drive/decoder_model.png')

pwd

def decode_sequence(input_seq):
    indices=[]
    target=[]
    newtarget=[]
    #states_value = encoder_model.predict(input_seq)
    #states_value=np.reshape(states_value, (32,2,1,300))
    #target_seq = np.zeros((32,1))
    # z=[]
    # for p in range(2):
    #   z.append([])
    #   for q in range(32):
    #     z[p].append(states_value[p][q])
    # print(len(z))
    # print(len(z[0]))
    # print(np.shape(z[1]))
    #target_seq[0, 0] = word2ind_tar['sos']

    for j in range(constant_batch_size):
      target.append([])
      #print(j)
      stop_condition = False
      states_value = encoder_model.predict(input_seq,batch_size=constant_batch_size)

      #print(np.shape(states_value))
      target_seq = np.zeros((constant_batch_size,1))
      target_seq[j, 0] = word2ind_tar['sos']
      # target_seq1=target_seq[j, 0]
      # print(target_seq1.shape)
      k=0

      while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value,batch_size=constant_batch_size)

        #print(output_tokens.shape)

        #for i in range(len(output_tokens[-2])):
        #integers = np.argmax(output_tokens[j])
        #print(output_tokens)
        integers = np.argmax(output_tokens[j])

        #print(integers)
        #stop_condition = True

        #i=i+1
        if integers==0:
          word="PADDING"
        else:
          integers=integers
          word = ind2word_tar[integers]
          #print(word)
        target[j].append(word)
        #print(target)
        #stop_condition = True
        if (word == 'eos' or len(target[j]) > 15):
            stop_condition = True

        if word is None:
 	 	        break
        #  # Update the target sequence (of length 1).
        target_seq = np.zeros((constant_batch_size,1))
        target_seq[j, 0] = integers
        #if target_train[j][k] != "eos":
        # if target_tokens_test[j][k] != "eos":
        #   k=k+1
        #print(k)
        #target_seq[j, 0] = word2ind_tar[target_train[j][k]]
        # target_seq[j, 0] = word2ind_tar[target_tokens_test[j][k]]
        # Update states
        states_value = [h, c]
      #print(target)
    #print(target)

    #target[j].append(word)

   # print(len(target[0]))
    for i in range(len(target)):
      newtarget.append([])
      for ele in target[i]:
        b=ele
        if b=='PADDING':
         del(b)
        else:
         newtarget[i].append(b)
# print(newtarget)



    return newtarget#decode_sentence
    #return target#decode_sentence

decoded_sentence1=decode_sequence([np.array(input_set_X1[0:128]),np.array(input_set_X2[0:128]),np.array(input_set_X3[0:128]),np.array(input_set_X4[0:128]),np.array(input_set_X5[0:128])])

print(decoded_sentence1)

decoded_sentence=decode_sequence([np.array(input_set_X1_test),np.array(input_set_X2_test),np.array(input_set_X3_test),np.array(input_set_X4_test),np.array(input_set_X5_test)])

print(decoded_sentence)

#ot training history
from matplotlib import pyplot

pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

print(history.history['loss'])
print(history.history['val_loss'])

output=model.predict([np.array(input_set_X1_test),np.array(input_set_X2_test),np.array(input_set_X3_test),np.array(input_set_X4_test),np.array(input_set_X5_test),np.array(input_set_X1_target_test)],batch_size=constant_batch_size,verbose=1)

output1=model.predict([np.array(input_set_X1),np.array(input_set_X2),np.array(input_set_X3),np.array(input_set_X4),np.array(input_set_X5),np.array(input_set_X1_target)],batch_size=constant_batch_size,verbose=1)

import numpy as np

from numpy import argmax
indices=[]
#target = list()
target1=[]
for j in range(len(output1)):
  target1.append([])
  for i in range(len(output1[-2])):
    integers = np.argmax(output1[j][i])
    # if integers==vocab_size_h-1:
    #     integers=integers
    # else:
    #     integers=integers+1
    if integers==0:
       #integers=integers+1
        word="PADDING"
    else:
       integers=integers
       word = ind2word_tar[integers]
    if word is None:
 	 	     break
    #target[j].append(word)
    target1[j].append(word)

    #a.replace('"', '')
    #target[j].append(word)

    #target[j]
print(target1)

import numpy as np

from numpy import argmax
indices=[]
#target = list()
target=[]
for j in range(len(output)):
  target.append([])
  for i in range(len(output[-2])):
    integers = np.argmax(output[j][i])
    # if integers==vocab_size_h-1:
    #     integers=integers
    # else:
    #     integers=integers+1
    if integers==0:
       #integers=integers+1
        word="PADDING"
    else:
       integers=integers
       word = ind2word_tar[integers]
    if word is None:
 	 	     break
    #target[j].append(word)
    target[j].append(word)

    #a.replace('"', '')
    #target[j].append(word)

    #target[j]
print(target)

print(len(target))

#a=target[5]
#print(a)
newtarget1=[]
#z=max_length_tr-1
for i in range(len(target1)):
  newtarget1.append([])
  for ele in target1[i]:
    b=ele
    if b=='PADDING':
     del(b)
    else:
      newtarget1[i].append(b)
print(newtarget1)
#   if ele=='0,0':
#       a.remove(ele)
#   else:
#     ele=ele
# #      #z=z-1
# print(a)



#a=target[5]
#print(a)
newtarget=[]
#z=max_length_tr-1
for i in range(len(target)):
  newtarget.append([])
  for ele in target[i]:
    b=ele
    if b=='PADDING':
     del(b)
    else:
      newtarget[i].append(b)
print(newtarget)
#   if ele=='0,0':
#       a.remove(ele)
#   else:
#     ele=ele
# #      #z=z-1
# print(a)

!pip install -U nltk

from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
#from nltk.translate.meteor_score import meteor_score
import nltk
import nltk.translate.gleu_score as gleu
#import nltk.translate.meteor_score.meteor_score

smoother = SmoothingFunction()
def evaluate_model(eng,hin,target):
      actual, predicted = list(),list()
      bb1=0
      bb2=0
      bb3=0
      bb4=0
      gleu_list=[]
      #gleu1=0
      c=0

      for i in range(128):
        raw_target, raw_src, tar  = hin[i],eng[i],target[i]
        tar=tar[0:-1]
        raw_target=raw_target[1:-1]
        raw_src=raw_src[1:-1]
        #tar=[tar]
        #raw_target=[raw_target]
        #raw_src=raw_src[1:-1]
        if i < 128:
		        print('src=%s, target=%s, pred=%s' % (raw_src, raw_target, tar))


  #     actual.append(raw_target)
  #     predicted.append(tar)
  # print(actual)
  # print(predicted)
        # score_ref_a = gleu.sentence_gleu([raw_target], tar)
        # print("Hyp and ref_a are the same: {}".format(score_ref_a))
        # wer_score(raw_target, tar, print_matrix=True)
        try:
          bb1+=(sentence_bleu([raw_target],tar,weights=(1,0,0,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(1,0,0,0), smoothing_function=smoother.method7))/2
          print(bb1)
          #meteor+=nltk.translate.meteor_score.meteor_score([raw_target], "tar")
          bb2+=(sentence_bleu([raw_target],tar,weights=(0,1,0,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,1,0,0), smoothing_function=smoother.method7))/2
          print(bb2)
          bb3+=(sentence_bleu([raw_target],tar,weights=(0,0,1,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,0,1,0), smoothing_function=smoother.method7))/2
          print(bb3)
          bb4+=(sentence_bleu([raw_target],tar,weights=(0,0,0,1), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,0,0,1), smoothing_function=smoother.method7))/2
          print(bb4)
          gleu1=gleu.sentence_gleu([raw_target], tar,min_len=1, max_len=1)
          print(gleu1)
          gleu_list.append(gleu1)
          #z=nltk.translate.chrf_score.chrf_precision_recall_fscore_support([raw_target], tar, 1, beta=3.0, epsilon=1e-16)
          #print(z)
        except ZeroDivisionError:
          pass
        # c+=1
        # if c%4==0:
          # print(c)
          # print(raw_src)
          # print(raw_target)
          # print(tar)
      bleu1=bb1*100/128
      bleu2=bb2*100/128
      bleu3=bb3*100/128
      bleu4=bb4*100/128
      gleu_avg=sum(gleu_list)/len(gleu_list)
      return bleu1,bleu2,bleu3,bleu4,gleu_avg

from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu

references = ['', '', '', ':', '', '', '']
candidates = ['', '', '', ':', '', '', '']
score = corpus_bleu(references, candidates)
print(score)



###day 11/06/21###with 1280 sentences for with san_eng new dataset san dep removed only eng dep

source_train=eng_htokens[0:128]

target_train=hin_etokens[0:128]

#source_tokens_test=san_test[0:128]

source_tokens_test=sanlst[0:128]

#target_tokens_test=hin_test[0:128]

target_tokens_test=hinlst[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######

###day 11/06/21###with 1280 sentences for with san_eng new dataset(5120-6400) san dep removed

source_train=eng_htokens[120:248]

target_train=hin_etokens[120:248]

source_tokens_test=san_test[0:128]

target_tokens_test=hin_test[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######



###day 11/06/21###with 1280 sentences for with san_eng new dataset(6400-7680) san dep removed

source_train=eng_htokens[1400:1528]

target_train=hin_etokens[1400:1528]

source_tokens_test=san_test[0:128]

target_tokens_test=hin_test[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######

0,1280):#len(corpus_pad_tr)
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840)

target_train=target_tokens[0:1280]

target_train=target_tokens[2560:3840]

target_train=target_tokens[1280:2560]

source_train=source_tokens[0:1280]

source_train=source_tokens[1280:2560]

source_train=source_tokens[2560:3840]

source_tokens_test=san_test

target_tokens_test=hin_test

print(target_tokens[100])

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

b=evaluate_model(source_train,target_train,decoded_sentence1)
print("bleu_score",b)

b=evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)
print("bleu_score",b)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

print("bleu_score",b)

"""Rough work"""

a=gauss_2d(0,1)

print(a)

inp = [[[10,0,0.1], [20,1,1.1], [30,2,2.1], [40,3,3.1], [50,4,4.1]],
       [[100,0.5,0.6], [200,1.5,1.6], [300,2.5,2.6], [400,3.5,3.6], [500,4.5,4.6]]]

tf.gather_nd(np.array(inp), [[[0, 1],[0, 2],[0,3],[0,0],[0,4]],
                             [[1, 1],[1, 2],[1,3],[1,4],[1,0]]])