# -*- coding: utf-8 -*-
"""Dependency_Parsing_ZSTsa_hi_only_engdep_25GB RAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLcjvLVumWFbKAYBXlGmq7KSeY1Mm1p0

# New Section
"""

from google.colab import drive
drive.mount('/content/drive')



from google.colab import drive
drive.mount('/content/drive')

pwd

cd drive/My Drive

cd-

!pip install indicnlp

import indicnlp
from indicnlp.tokenize import indic_tokenize
tokens=[]
lines_num=[]
length=[]
dup=[]
lines_num1=[]
with open("drive/MyDrive/hin_test.txt","r") as xh:

  hin_lines=xh.readlines()
  for i in range(len(hin_lines)):
   tokens.append([])
   for t in indic_tokenize.trivial_tokenize(hin_lines[i]):
    tokens[i].append(t)
    if len(tokens[i])>70:
      lines_num.append(i)
      length.append(len(tokens[i]))
for z in lines_num:
  if z not in dup:
    lines_num1.append(z)
    dup.append(z)

#print(length)
length.sort()
#print(length[-2])
print(length)
print(lines_num)
print(tokens)
print(lines_num1)

lin=[]
   for t in indic_tokenize.trivial_tokenize(hin_lines[1]):
     #print(len(t))
     lin.append(t)
print(len(lin))
print(lin)

print(tokens[0])

print(hin_lines[4].split(" "))

print(len(hin_lines[4].split(" ")))

xa=open("drive/MyDrive/hin_test.txt","r")

xh=open("drive/MyDrive/hin_test_new.txt","w")

file_lines = {}
initial_line = 0
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in lines_num1:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')

xa=open("drive/MyDrive/san_test1.txt","r")

xh=open("drive/MyDrive/san_test1_new.txt","w")

file_lines = {}
initial_line = 0
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in lines_num1:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')

with open("drive/MyDrive/hin_efiltered1.txt","r") as xw:

   hin_e=xw.readlines()
   #z=1000
   k=0
   line_num=[]
   for lines in hin_e:
    k=k+1
    words=lines.split(" ")
    for i in words:
    #for word in lines:
      if i == "HEAD.":
        print(lines)
        #print(k)
        line_num.append(k)

    # z=z-1
    # if z<=0:
    #   break
print(line_num)

print(len(line_num))

xa=open("drive/MyDrive/hin_efiltered1.txt","r")

xh=open("drive/MyDrive/hin_efiltered1_new.txt","w")

file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in line_num:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')
        #lines_seen.add(line_content)

xa=open("drive/MyDrive/eng_hfiltered1_tok.txt","r")

xh=open("drive/MyDrive/eng_hfiltered1_tok_new.txt","w")

file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in line_num:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')
        #lines_seen.add(line_content)

with open("drive/MyDrive/hin_efiltered1_new.txt","r") as xh:
  length=[]
  lines_num=[]
  k=0
  hin_lines=xh.readlines()
  for lines in hin_lines:
    k=k+1
    words=lines.split(" ")
    #print(len(words))
    length.append(len(words))
    if len(words)>70:
      lines_num.append(k)
print(length)
length.sort()
print(length[-2])
print(length)
print(lines_num)

xa=open("drive/MyDrive/hin_efiltered1_new.txt","r")

xh=open("drive/MyDrive/hin_efiltered1_new1.txt","w")

file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in lines_num:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')
        #lines_seen.add(line_content)

xa=open("drive/MyDrive/eng_hfiltered1_tok_new.txt","r")

xh=open("drive/MyDrive/eng_hfiltered1_tok_new1.txt","w")

file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
for line_number, line_content in file_lines.items():
  if line_number not in lines_num:  #duplicate_line list from eng list above
        xh.write(line_content + '\n')
        #lines_seen.add(line_content)

with open("drive/MyDrive/hin_efiltered1_new1.txt","r") as xh:
  length=[]
  lines_num=[]
  k=0
  hin_lines=xh.readlines()
  for lines in hin_lines:
    k=k+1
    words=lines.split(" ")
    #print(len(words))
    length.append(len(words))
    if len(words)>70:
      lines_num.append(k)
print(length)
length.sort()
print(length[-2])
print(length)
print(lines_num)

with open("drive/MyDrive/eng_hfiltered1_tok_new1.txt","r") as xh:
  length=[]
  lines_num=[]
  k=0
  hin_lines=xh.readlines()
  for lines in hin_lines:
    k=k+1
    words=lines.split(" ")
    #print(len(words))
    length.append(len(words))
    if len(words)>70:
      lines_num.append(k)
print(length)
length.sort()
print(length[-2])
print(length)
print(lines_num)

with open("drive/MyDrive/san_efiltered1_tok.txt","w") as xw:

  with open("drive/MyDrive/san_efiltered1.txt") as xh:
   san_e=xh.readlines()
   for lines in san_e:
     xw.write("en "+ lines)

with open("drive/MyDrive/eng_hfiltered1_tok.txt","w") as xw:

  with open("drive/MyDrive/eng_hfiltered1.txt") as xh:
   san_e=xh.readlines()
   for lines in san_e:
     xw.write("hi "+ lines)

#testfile
with open("drive/MyDrive/hi_sa_test.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
    with open("drive/MyDrive/hin_test.txt","w") as zh:
          with open("drive/MyDrive/san_test.txt","w") as en:
              xlines = xh.readlines()
              i=0
              #for lines in xlines:
              for j in range(188):
                 i=i+1
                 splitlines=xlines[j].split("\t")
                #  print(splitlines[1])
                #  print(i)
                 hindata=splitlines[0]
                 engdata=splitlines[1]
                 zh.write(hindata + "\n")
                 en.write(engdata)

#tokenizarion san test file
with open("drive/MyDrive/san_test1.txt","w") as xw:

  with open("drive/MyDrive/san_test.txt") as xh:
   san_e=xh.readlines()
   for lines in san_e:
     xw.write("hi "+ lines)

with open("drive/MyDrive//hi_eng_final.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
    with open("drive/MyDrive/hin_e.txt","w") as zh:
         # with open("drive/MyDrive/eng_s.txt","w") as en:
              xlines = xh.readlines()
              i=0
              for lines in xlines:
                 i=i+1
                 splitlines=lines.split("\t")
                 hindata=splitlines[0]
                 #engdata=splitlines[1]
                 zh.write(hindata + "\n")

with open("drive/MyDrive/hi_eng_final.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
    with open("drive/MyDrive/eng_h.txt","w") as en:
      #Read first file
      #for line in enumerate xh:
      xlines = xh.readlines()
      #print(xlines1561649.split("\t")
      i=0
      for lines in xlines:
        i=i+1
        splitlines=lines.split("\t")
        if "\n" in splitlines[1]:
          sandata=splitlines[1]
          en.write(sandata)
        else:
          sandata=splitlines[1]
          en.write(sandata+ "\n")
        #print(splitlines)
    #     engdata=splitlines[1]
    #     #print(lines[])
         # en.write(engdata)
      print(i)
      en.close()
      xh.close()

with open("drive/MyDrive/en_safinal.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
    with open("drive/MyDrive/san_e.txt","w") as zh:
          with open("drive/MyDrive/eng_s.txt","w") as en:
              xlines = xh.readlines()
              i=0
              for lines in xlines:
                 i=i+1
                 splitlines=lines.split("\t")
                 engdata,sandata=splitlines[0],splitlines[1]
                 #engdata=splitlines[1]
                 zh.write(sandata)
                 en.write(engdata +"\n")
              print(i)
              zh.close()
              xh.close()
              en.close()

with open("drive/MyDrive/en_safinal.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
    with open("drive/MyDrive/san_e.txt","w") as en:
      #Read first file
      #for line in enumerate xh:
      xlines = xh.readlines()
      #print(xlines1561649.split("\t")
      i=0
      for lines in xlines:
        i=i+1
        splitlines=lines.split("\t")
        if "\n" in splitlines[1]:
          sandata=splitlines[1]
          en.write(sandata)
        else:
          sandata=splitlines[1]
          en.write(sandata+ "\n")
        #print(splitlines)
    #     engdata=splitlines[1]
    #     #print(lines[])
         # en.write(engdata)
      print(i)
      en.close()
      xh.close()

#counting englines in eng textfile of parallel eng-hin
with open("drive/MyDrive/eng_h.txt") as xh:
  englines=xh.readlines()
print(len(englines))

#counting hindilines in hin textfile of parallel eng-hin
with open("drive/MyDrive/hin_e.txt") as xh:
  hinlines=xh.readlines()
print(len(hinlines))

#counting englines in eng textfile of parallel eng-hin
with open("drive/MyDrive/hi_eng_final.txt") as xh:
  englines=xh.readlines()
z=0
for i in (englines):
  z=z+1
print(z)



!pip install indic-nlp-library

#removing specila characters in eng_s source text
import re
with open("drive/MyDrive/eng_s1.txt",'w') as xs:
  lines = open("drive/MyDrive/eng_s.txt", encoding='UTF-8').read().strip().split('\n')
  for words in lines:
    words=words.lower()
    engdata=re.sub(r"[^a-zA-Z0-9]+", ' ', words)
    xs.write(engdata + "\n")
    #print(word)
    #xlines = xh.readlines()
  # for k in lines:
  #

  #engdata=preprocess_sentence(lines[i])
  #print(engdata)
       #xs.write(engdata)

with open("drive/MyDrive/eng_s1.txt",'r') as xs:
  eng=xs.readlines()

  print(len(eng))
  # for line in eng:
  #   i=i+1
  # print(i)

#removing specila characters in eng_h source text
import re
with open("drive/MyDrive/eng_h1.txt",'w') as xs:
  lines = open("drive/MyDrive/eng_h.txt", encoding='UTF-8').read().strip().split('\n')
  for words in lines:
    words=words.lower()
    engdata=re.sub(r"[^a-zA-Z0-9]+", ' ', words)
    xs.write(engdata + "\n")
    #print(word)
    #xlines = xh.readlines()
  # for k in lines:
  #

  #engdata=preprocess_sentence(lines[i])
  #print(engdata)
       #xs.write(engdata)

with open("drive/MyDrive/eng_h1.txt",'r') as xs:
  eng=xs.readlines()

  print(len(eng))
  # for line in eng:
  #   i=i+1
  # print(i)

with open("drive/MyDrive/hin.txt",'r') as xs:
  hin=xs.readlines()

  print(len(hin))
  # for line in eng:
  #   i=i+1

  # print(i)

#not using now
#removing eng character from hindi text
import re
l=open("/content/drive/My Drive/hin_e.txt","r")
#a= open("/content/drive/My Drive/hin1.txt","w+")
#b= open("/content/drive/My Drive/hin_new.txt","w+")
#d=open("/content/drive/My Drive/eng_clean.txt","r")
#eng=d.readlines()
i=0
j=0
linenumber=[]
for string in l:
      i=i+1
      text=re.search(r'[a-zA-Z]', string)
      if text:
        #a.write(string + str(i))
        linenumber.append(str(i))
        #print(linenumber)

       # for line in eng:
      else:
        j=j+1
        #b.write(string)

print(i)
print(j)
print(linenumber)
print(len(linenumber))

print(linenumber)
print(len(linenumber))

print(linenumber)

#sample code
d=open("drive/MyDrive/hin_n.txt","r")
content1=d.readlines()
linenumber1=[1,2,3]
file_lines1 = {}
initial_line = 1
#print(linenumber1[0])
for line in content1:
   file_lines1[initial_line] = line.strip()
   initial_line += 1
i=1
f = open("drive/MyDrive/hin_new.txt", "w")
g = open("drive/MyDrive/hin_new1.txt", "w")
for line_number, line_content in file_lines1.items():
   res = any(lin == line_number for lin in linenumber1 )
   #print(res)
   if res:
     f.write('{}\n'.format(line_content))
     print(res,line_number)
   else:
     g.write('{}\n'.format(line_content))
     print(res,line_number)

#limiting hindi line to one lakh
with open("drive/MyDrive/hin_e.txt") as xh:
  with open("drive/MyDrive/hin_e_red.txt",'w') as xs:
    xlines = xh.readlines()
    for i in range(100000):
      #hinred=xlines[i]
      xs.write(xlines[i])

with open("drive/MyDrive/hin_e_red.txt") as xh:
  #with open("drive/MyDrive/hin_e_red.txt",'w') as xs:
    xlines = xh.readlines()
print(len(xlines))

##limiting englines line to one lakh

with open("drive/MyDrive/eng_h1.txt") as xh:
  with open("drive/MyDrive/eng_h1_red.txt",'w') as xs:
    xlines = xh.readlines()
    for i in range(100000):
      engred=xlines[i]
      xs.write(engred)

#removing duplicates line fro eng text
lines_seen = set()  # holds lines already seen
outfile = open('drive/MyDrive/eng_h1_red_dupre', "w")
outfile1 = open('drive/MyDrive/eng_h1_dup_lines', "w")

infile = open('drive/MyDrive/eng_h1_red.txt', "r")
#print "The file bar.txt is as follows"
file_lines = {}
initial_line = 1
duplicate_line=[]
for line in infile:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
  if line_content not in lines_seen:  # not a duplicate
        outfile.write(line_content + '\n')
        lines_seen.add(line_content)
  else:
        outfile1.write(str(line_number)+"\n")
        duplicate_line.append(line_number)
#print(lines_seen)
outfile.close()

#removing duplicates line fro hin text , same lines removed as eng file above
lines_seen = set()  # holds lines already seen
outfile = open('drive/MyDrive/hin_e_red_dupre.txt', "w")
infile = open('drive/MyDrive/hin_e_red.txt', "r")
file_lines = {}
initial_line = 1
duplicate_line_hin=[]
for line in infile:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
  if line_number not in duplicate_line:  #duplicate_line list from eng list above
        outfile.write(line_content + '\n')
        lines_seen.add(line_content)
  else:
        #outfile1.write(str(line_number)+"\n")
        duplicate_line_hin.append(line_number)
#print(lines_seen)
outfile.close()


# print "The file foo.txt is as follow

#removing duplicates line fro san_e text
lines_seen = set()  # holds lines already seen
outfile = open('drive/MyDrive/san_e_red_dupre', "w")
#outfile1 = open('drive/MyDrive/eng_h1_dup_lines', "w")

infile = open('drive/MyDrive/san_e.txt', "r")
#print "The file bar.txt is as follows"
file_lines = {}
initial_line = 1
duplicate_line=[]
for line in infile:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
  if line_content not in lines_seen:  # not a duplicate
        outfile.write(line_content + '\n')
        lines_seen.add(line_content)
  else:
        #outfile1.write(str(line_number)+"\n")
        duplicate_line.append(line_number)
#print(lines_seen)
outfile.close()

print(duplicate_line)

print(duplicate_line)

print(len(duplicate_line))

#removing duplicates line fro eng text , same lines removed as san file above
lines_seen = set()  # holds lines already seen
outfile = open('drive/MyDrive/eng_s1_red_dupre.txt', "w")
infile = open('drive/MyDrive/eng_s1.txt', "r")
file_lines = {}
initial_line = 1
duplicate_line_hin=[]
for line in infile:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
  if line_number not in duplicate_line:  #duplicate_line list from eng list above
        outfile.write(line_content + '\n')
        lines_seen.add(line_content)
  else:
        #outfile1.write(str(line_number)+"\n")
        duplicate_line_hin.append(line_number)
#print(lines_seen)
outfile.close()


# print "The file foo.txt is as follow



len(duplicate_line_hin)

outfile = open('drive/MyDrive/hin_dup_remo.txt')
lines=outfile.readlines()

len(lines)

print(len(duplicate_line_hin))

print(duplicate_line_hin)

#removing lines which have one or less than ome word for hindi
hin=open("drive/MyDrive/hin_e_red_dupre.txt","r")
hin_n=open("drive/MyDrive/hin_efiltered.txt","w")
hin_n2=open("drive/MyDrive/hin_efiltered2.txt","w")

file_lines1 = {}
initial_line = 1
singleword_linenumber=[]
for line in hin:
    file_lines1[initial_line] = line.strip()
    initial_line += 1
for line_number1, line_content1 in file_lines1.items():
   if (len(line_content1.split()))>1:
        hin_n.write(line_content1+'\n')
   else:
        hin_n2.write(line_content1+'\n')
        singleword_linenumber.append(line_number1)

print(singleword_linenumber)

len(singleword_linenumber)

print(len(singleword_linenumber))

#removing the same lines those were removed from hindi file with one word
xa=open("drive/MyDrive/eng_h1_red_dupre.txt","r")
xb=open("drive/MyDrive/eng_hfiltered.txt","w")
file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
    if line_number not in singleword_linenumber:  # not a duplicate
        xb.write(line_content + '\n')
    else:
        eng_singlewordlinenumber.append(line_number)

#print(lines_seen)
xb.close()

#removing lines which have one or less than ome word from eng_h
hin=open("drive/MyDrive/eng_hfiltered.txt","r")
hin_n=open("drive/MyDrive/eng_h_efiltered1_1.txt","w")
hin_n2=open("drive/MyDrive/hin_efiltered2.txt","w")

file_lines1 = {}
initial_line = 1
singleword_linenumber=[]
for line in hin:
    file_lines1[initial_line] = line.strip()
    initial_line += 1
for line_number1, line_content1 in file_lines1.items():
   if (len(line_content1.split()))>1:
        hin_n.write(line_content1+'\n')
   else:
        hin_n2.write(line_content1+'\n')
        singleword_linenumber.append(line_number1)

#removing the same lines those were removed from eng file with one word from hin file
xa=open("drive/MyDrive/hin_efiltered.txt","r")
xb=open("drive/MyDrive/hin_efiltered1_1.txt","w")
file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
    if line_number not in singleword_linenumber:  # not a duplicate
        xb.write(line_content + '\n')
    else:
        eng_singlewordlinenumber.append(line_number)

#print(lines_seen)
xb.close()

#removing lines which have one or less than ome word for sanskrit_e
hin=open("drive/MyDrive/san_e_red_dupre.txt","r")
hin_n=open("drive/MyDrive/san_efiltered.txt","w")
#hin_n2=open("drive/MyDrive/hin_efiltered2.txt","w")

file_lines1 = {}
initial_line = 1
singleword_linenumber=[]
for line in hin:
    file_lines1[initial_line] = line.strip()
    initial_line += 1
for line_number1, line_content1 in file_lines1.items():
   if (len(line_content1.split()))>1:
        hin_n.write(line_content1+'\n')
   else:
        hin_n2.write(line_content1+'\n')
        singleword_linenumber.append(line_number1)

#removing the same lines those were removed from san_e file with one word fron eng_s
xa=open("drive/MyDrive/eng_s1_red_dupre.txt","r")
xb=open("drive/MyDrive/eng_sfiltered.txt","w")
file_lines = {}
initial_line = 1
eng_singlewordlinenumber=[]
for line in xa:
    file_lines[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines.items():
    if line_number not in singleword_linenumber:  # not a duplicate
        xb.write(line_content + '\n')
    else:
        eng_singlewordlinenumber.append(line_number)

#print(lines_seen)
xb.close()

#removing lines which have one or less than ome word for eng_s
eng=open("drive/MyDrive/eng_sfiltered.txt","r")
eng_n=open("drive/MyDrive/eng_sfiltered1.txt","w")
#eng_n2=open("drive/MyDrive/eng_filtered2.txt","w")

file_lines2 = {}
initial_line = 1
eng_single_word_linenumber=[]
for line in eng:
    file_lines2[initial_line] = line.strip()
    initial_line += 1
for line_number1, line_content1 in file_lines2.items():
   if (len(line_content1.split()))>1:
        eng_n.write(line_content1+ '\n')
   else:
        #eng_n2.write(line_content1+'\n')
        eng_single_word_linenumber.append(line_number1)

#removing the same lines those were removed from eng file from san_e with one word
xa=open("drive/MyDrive/san_efiltered.txt","r")
xb=open("drive/MyDrive/san_efiltered1.txt","w")
file_lines3 = {}
initial_line = 1
hin_singlewordlinenumber=[]
for line in xa:
    file_lines3[initial_line] = line.strip()
    initial_line += 1
#print(linenumber)

for line_number, line_content in file_lines3.items():
    if line_number not in eng_single_word_linenumber:  # not a duplicate
        xb.write(line_content + '\n')
    else:
        hin_singlewordlinenumber.append(line_number)

#print(lines_seen)
xb.close()

######preparing san-eng new dataset####

with open("drive/MyDrive/Sanskrit-Hindi-Machine-Translation/parallel-corpus/sanskrit-english/parallel_data.txt") as xh:
  #with open('Ubuntu.hi-sa.hi') as yh:
  with open("drive/MyDrive/eng_s_new.txt","w") as en:
    #with open("drive/MyDrive/san_e_new.txt","w") as sa:
      xlines = xh.readlines()
      i=0
      for lines in xlines:
        i=i+1
        splitlines=lines.split("\t")
        # sandata=splitlines[0]
        # sa.write(sandata+"\n")
        if "\n" in splitlines[1]:
          engdata=splitlines[1]
          en.write(engdata)
        else:
          engdata=splitlines[1]
          en.write(engdata+ "\n")
        #print(splitlines)
    #     engdata=splitlines[1]
    #     #print(lines[])
         # en.write(engdata)
      print(i)
      en.close()
      #sa.close()
      xh.close()

infile = "drive/MyDrive/eng_s_new.txt"
outfile = "drive/MyDrive/eng_s_new2.txt"

delete_list = ["START_", "_END"]
with open(infile) as fin, open(outfile, "w+") as fout:
    for line in fin:
        for word in delete_list:
            line = line.replace(word, "")
        fout.write(line)

with open( "drive/MyDrive/eng_s_new2.txt","r") as fin:
  with open( "drive/MyDrive/eng_s_new3.txt","w") as fout:
    lines=fin.readlines()
    x=0
    for line in lines:
      x=x+1
      fout.write(line)
      if x==10000:
        break

with open( "drive/MyDrive/san_e_new.txt","r") as fin:
  with open( "drive/MyDrive/san_e_new2.txt","w") as fout:
    lines=fin.readlines()
    x=0
    for line in lines:
      x=x+1
      fout.write("en " + line)
      if x==10000:
        break

#san_e_new2 for sanskrit in Sanskrit-english
#eng_s_new3 for eng in Sanskrit-english

#eng_hfiltered1_tok_new2.txt
#hin_efiltered1_new2.txt

!pip install fasttext

import fasttext
model = fasttext.train_unsupervised('drive/MyDrive/new_eng.txt')

model.save_model("drive/MyDrive/new_eng.bin")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext rpy2.ipython

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages("udpipe")

pwd

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# udmodel <- udpipe_download_model(language = "english")
# udmodel <- udpipe_load_model(file = udmodel$file_model)
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# udmodel <- udpipe_download_model(language = "english")
# udmodel <- udpipe_load_model(file = udmodel$file_model)
# #txt<-[]
# fileName <- "drive/MyDrive/eng_hfiltered1_tok_new3.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #vector <- c()
# #print(linn[0])
# # for (i in 1:length(linn)){
#   #   vector[i]<-linn[i]
# 
#   # }
# x <- udpipe_annotate(udmodel, x = linn)
#     #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #x
# cat(x$conllu, file = "drive/MyDrive/eng_h.conllu")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# udmodel <- udpipe_download_model(language = "hindi")
# udmodel <- udpipe_load_model(file = udmodel$file_model)
# #txt<-[]
# fileName <- "drive/MyDrive/hin_filtered1.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #vector <- c()
# #print(linn[0])
# # for (i in 1:length(linn)){
#   #   vector[i]<-linn[i]
# 
#   # }
# x <- udpipe_annotate(udmodel, x = linn)
#     #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #x
# cat(x$conllu, file = "drive/MyDrive/hin_my_new.conllu")

pwd

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# dl <- udpipe_download_model(language = "sanskrit")
# str(dl)

ls

# Commented out IPython magic to ensure Python compatibility.
# 
# %%R
# library(udpipe)
# dl <- udpipe_download_model(language = "sanskrit", udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0")
# udmodel_sanskrit <- udpipe_load_model(file = dl$file_model)
# fileName <- "drive/MyDrive/san_test1_new1.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #for (i in 1:8){
# text<-linn[1]
# print(text[1])
# # print(length(text))
# x <- udpipe_annotate(udmodel_sanskrit, x=text)   #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #Encoding(x$conllu)
# cat(x$conllu, file = "san_my1.conllu")

# Commented out IPython magic to ensure Python compatibility.
# #for sanskrit test file
# %%R
# library(udpipe)
# dl <- udpipe_download_model(language = "sanskrit", udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0")
# udmodel_sanskrit <- udpipe_load_model(file = dl$file_model)
# #txt<-[]
# fileName <- "drive/MyDrive/san_test1_new1.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #vector <- c()
# #print(linn[0])
# # for (i in 1:length(linn)){
# #      x<-linn[i]
# 
#   # }
# x <- udpipe_annotate(udmodel_sanskrit, x = linn)
#     #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #Encoding(x$conllu)
# cat(x$conllu, file = "drive/MyDrive/san_test.conllu")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# dl <- udpipe_download_model(language = "sanskrit", udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0")
# udmodel_sanskrit <- udpipe_load_model(file = dl$file_model)
# #txt<-[]
# fileName <- "drive/MyDrive/san_efiltered3_tok.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #vector <- c()
# #print(linn[0])
# # for (i in 1:length(linn)){
# #      x<-linn[i]
# 
#   # }
# x <- udpipe_annotate(udmodel_sanskrit, x = linn)
#     #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #Encoding(x$conllu)
# cat(x$conllu, file = "drive/MyDrive/san_e.conllu")

# Commented out IPython magic to ensure Python compatibility.
# ####for new sanskrit train file
# %%R
# library(udpipe)
# dl <- udpipe_download_model(language = "sanskrit", udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0")
# udmodel_sanskrit <- udpipe_load_model(file = dl$file_model)
# #txt<-[]
# fileName <- "drive/MyDrive/san_e_new2.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# #vector <- c()
# #print(linn[0])
# # for (i in 1:length(linn)){
# #      x<-linn[i]
# 
#   # }
# x <- udpipe_annotate(udmodel_sanskrit, x = linn)
#     #Encoding(x$conllu)
# #x <- as.data.frame(x, detailed = TRUE)
# #Encoding(x$conllu)
# cat(x$conllu, file = "drive/MyDrive/san_e.conllu")

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(udpipe)
# fileName <- "drive/MyDrive/san.txt"
# conn <- file(fileName,open="r")
# linn <-readLines(conn)
# print(linn[[7]])

!pip install conllu

import conllu
import unicodedata, re
from collections import deque
import tensorflow as tf
from tensorflow import keras

import numpy as np
import pandas as pd
import re
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

import nltk
nltk.download('punkt')

def preorderTraversal(root):
    word_order = {} # as we don't get the original sentence token order using parsetree
    Stack = deque([])
    # 'Preorder'-> contains all the
    # visited nodes.
    Preorder =[]
    Preorder.append(root.token['form'])
    word_order[root.token['form']] = root.token['id']
    Stack.append(root)
    while len(Stack)>0:
        flag = 0
        if len((Stack[len(Stack)-1]).children)== 0:
            X = Stack.pop()
        else:
            Par = Stack[len(Stack)-1]
        for i in range(0, len(Par.children)):
            if Par.children[i].token['form'] not in Preorder:
                flag = 1
                Stack.append(Par.children[i])
                Preorder.append(Par.children[i].token['form'])
                word_order[Par.children[i].token['form']] = Par.children[i].token['id']
                break;
        if flag == 0:
            Stack.pop()
    # print(Preorder)
    return Preorder, word_order

def postorderTraversal(root):
    word_order = {} # as we don't get the original sentence token order using parsetree
    Postorder =[]
    if not root:
        return []
    Stack = [root]
    last = None

    while Stack:
        root = Stack[-1]
        if not root.children or last and (last in root.children):
            Postorder.append(root.token['form'])
            word_order[root.token['form']] = root.token['id']
            Stack.pop()
            last = root
        else:
            for children in root.children[::-1]:
                Stack.append(children)
    # print(Postorder)
    return Postorder, word_order

pwd

!pip install indic-nlp-library

!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git

import sys
from indicnlp import common

# The path to the local git repo for Indic NLP library
INDIC_NLP_LIB_HOME=r"indic_nlp_library"

# The path to the local git repo for Indic NLP Resources
INDIC_NLP_RESOURCES=r"indic_nlp_resources"

# Add library to Python path
sys.path.append(r'{}\src'.format(INDIC_NLP_LIB_HOME))

# Set environment variable for resources folder
common.set_resources_path(INDIC_NLP_RESOURCES)

!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

#for target test file (hindi test file)
import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/hin_test_new1.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
hindilines=hinditext_file.readlines()
print(len(hindilines))
tokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=188
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
for i in range(len(hindilines)):
  hindilines1="sos " + hindilines[i].strip("\n")+ " eos"
  #print(hindilines1)
  tokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    tokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
hin_test=tokens
print(hin_test)
#print(hin_sent_tok)

print(len(hin_test))

#for source test file (sankrit test file)
import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/san_test1_new1.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
sanlines=hinditext_file.readlines()
tokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=188
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
for i in range(len(sanlines)):
  hindilines1=sanlines[i].strip("\n")
  #print(hindilines1)
  tokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    tokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
san_test=tokens
print(san_test)
#print(hin_sent_tok)

num_lines=10000

import indicnlp
from indicnlp.tokenize import indic_tokenize
from indicnlp.tokenize import sentence_tokenize

hinditext_file = open("drive/MyDrive/hin_efiltered1_new3.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
hindilines=hinditext_file.readlines()
hin_etokens=[]
tokens1=[]
hin_sent_tok=[]
#red_count=num_lines
#print('Input String: {}'.format(indic_string))
#print('Tokens: ')
j=0
k=5000
for i in range(j,k):

#for i in range(len(hindilines)):
  hindilines1="sos " + hindilines[i].strip("\n")+ " eos"
  #print(hindilines1)
  hin_etokens.append([])
  for t in indic_tokenize.trivial_tokenize(hindilines1):
    hin_etokens[i].append(t)

  hin_sent_tok.append(sentence_tokenize.sentence_split(hindilines1, lang='hi'))
  # red_count-=1
  # if red_count<=0:
  #   break;
#hin_etokens=tokens
print(hin_etokens)
print(len(hindilines))

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
eng_sent_tokens=[]
eng_stokens=[]
engtext_file = open("drive/MyDrive/eng_sfiltered3.txt", "r", encoding="utf-8")
#engtext_file = open("drive/MyDrive/itihasa-main/data/train.en", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1="sos " + englines[i] + " eos"
  eng_stokens.append([])
  for t in word_tokenize(englines1):
    eng_stokens[i].append(t)
  eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(eng_stokens)
#print(eng_sent_tokens)

print(len(eng_stokens))

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
#eng_sent_tokens=[]
san_etokens=[]
#engtext_file = open("drive/MyDrive/itihasa-main/data/train.sn", "r", encoding="utf-8")
engtext_file = open("drive/MyDrive/san_efiltered3_tok.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1= "en "+ englines[i]
  san_etokens.append([])
  for t in word_tokenize(englines1):
    san_etokens[i].append(t)
  eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(san_etokens)
print(len(englines))

print(san_etokens[1])

print(len(san_etokens))

print(san_etokens[0])

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
#eng_sent_tokens=[]
eng_htokens=[]
engtext_file = open("drive/MyDrive/eng_hfiltered1_tok_new3.txt", "r", encoding="utf-8")
#hindilines=hinditext_file.read()
englines=engtext_file.readlines()
#red_count=num_lines
j=0
k=5000
for i in range(j,k):

#for i in range(len(englines)):
  englines1= englines[i]
  eng_htokens.append([])
  for t in word_tokenize(englines1):
    eng_htokens[i].append(t + ' ')
  #eng_sent_tokens.append(sent_tokenize(englines[i]))
  # red_count-=1
  # if red_count<=0:
  #   break;
print(eng_htokens)
print(len(englines))

target_train_tokens=[]
for i in eng_stokens:
  target_train_tokens.append(i)

for i in hin_etokens:
  target_train_tokens.append(i)

target_tokens=[]
for i in eng_stokens:
  target_tokens.append(i)
print(len(eng_sent_tokens))

print(len(target_tokens))

#target_tokens=[]
for i in hin_etokens:
  target_tokens.append(i)
print(len(hin_etokens))

for i in hin_test:
  target_tokens.append(i)

print(len(target_tokens))



data_file = open("drive/MyDrive/san_test.conllu", "r", encoding="utf-8")
all_corpus_text_test_s=[]
all_corpus_index_test_order_s=[]
reduce_count=148

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
  except Exception:
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-orderindex: ", pre_order_index_input, "\nPost-orderindex: ", post_order_index_input)

  all_corpus_text_test_s.append([normal_order,pre_order,post_order])
  all_corpus_index_test_order_s.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

####skip san dep parsing####

###save normal,pre,post as san sentence)
all_corpus_text_test_s=[]
for i in range(len(san_test)):
  all_corpus_text_test_s.append([san_test[i],san_test[i],san_test[i]])
#### storing index###Pre Post###

z=0
p=0
index0=[]
for i in san_test:

  index0.append([])
  for word in i:
    index=z
    index0[p].append(index)
    z=z+1
    if z==len(i):
      z=0
  p=p+1
index1=[]
for i in index0:
  array=np.array(i)
  index1.append(array)
all_corpus_index_test_order_s=[]
for i in index1:
  all_corpus_index_test_order_s.append([i,i])

#####storing index san text end ####

data_file = open("drive/MyDrive/san_e.conllu", "r", encoding="utf-8")
all_corpus_text_s=[]
all_corpus_index_order_s=[]
reduce_count=5000

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
  except Exception:
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-orderindex: ", pre_order_index_input, "\nPost-orderindex: ", post_order_index_input)

  all_corpus_text_s.append([normal_order,pre_order,post_order])
  all_corpus_index_order_s.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

#####storing san train data and indices for pre post begin###

####skip san dep parsing train san sentences####
#### storing index###Pre Post###
all_corpus_text_s=[]
for i in range(len(san_etokens)):
  all_corpus_text_s.append([san_etokens[i],san_etokens[i],san_etokens[i]])
z=0
p=0
index0=[]
for i in san_etokens:

  index0.append([])
  for word in i:
    index=z
    index0[p].append(index)
    z=z+1
    if z==len(i):
      z=0
  p=p+1
#all_corpus_index_test_order_s1=np.array
index1=[]
for i in index0:
  array=np.array(i)
  index1.append(array)
all_corpus_index_order_s=[]
for i in index1:
  all_corpus_index_order_s.append([i,i])

###end san train skip dep####

print(len(all_corpus_text_s))

print(all_corpus_index_order_s[0])

data_file = open("drive/MyDrive/eng_h.conllu", "r", encoding="utf-8")
all_corpus_text_e=[]
all_corpus_index_order_e=[]
reduce_count=5000

for tokentree in conllu.parse_tree_incr(data_file):
  print(tokentree)
  print(tokentree.print_tree())
  print(tokentree.metadata['text'])
  try:
    pre_order, normal_order  = preorderTraversal(tokentree)
  except Exception:
    all_corpus_text.append([None,None,None])
    all_corpus_index_order.append([None,None])
    continue
  post_order, _ = postorderTraversal(tokentree)
  normal_order = sorted(normal_order, key=normal_order.get) # sort values and get the key, i.e., words
  normal_order = [w.lower() for w in normal_order]
  pre_order = [w.lower() for w in pre_order]
  post_order = [w.lower() for w in post_order]

  print("Normal: ", normal_order, "\nPre-order: ", pre_order, "\nPost-order: ", post_order)

  pre_order_index_input = np.array([pre_order.index(item) for item in normal_order]).reshape(-1,)
  post_order_index_input = np.array([post_order.index(item) for item in normal_order]).reshape(-1,)
  print("\nPre-order-index: ", pre_order_index_input, "\nPost-order-index: ", post_order_index_input)

  all_corpus_text_e.append([normal_order,pre_order,post_order])
  all_corpus_index_order_e.append([pre_order_index_input,post_order_index_input])

  reduce_count-=1
  if reduce_count<=0:
    break

all_corpus_text_source=[]
for i in all_corpus_text_s:
  all_corpus_text_source.append(i)
print(len(all_corpus_text_s))# getting more sentences from san_e conllu##check later

for i in all_corpus_text_e:
  all_corpus_text_source.append(i)
print(len(all_corpus_text_e))

len(eng_htokens)

source_tokens=[]
for i in all_corpus_text_s:
  source_tokens.append(i)

for i in all_corpus_text_e:
  source_tokens.append(i)

for i in all_corpus_text_test_s:
  source_tokens.append(i)
print(len(all_corpus_text_test_s))

all_corpus_index_order=[]
for i in all_corpus_index_order_s:
  all_corpus_index_order.append(i)

for i in all_corpus_index_order_e:
  all_corpus_index_order.append(i)



len(all_corpus_text_source), len(all_corpus_index_order)

len(all_corpus_text_source), len(all_corpus_index_order)

#import itertools
import more_itertools as mit


vocab_tar=set(mit.collapse(target_tokens))
#print(flat_list_h)
# {'are', 'good', 'hello', 'hey', 'hi', 'how', 'jane', 'morning', 'top', 'you'}
# flat_list_h = list(itertools.chain(hindi_tokens))
# flat_list_h = list(itertools.chain(flat_list_h))
# vocab_h = sorted(set(flat_list_h))
print(len(vocab_tar))
word2ind_tar = {}
ind2word_tar = {}

ind=1
for word in vocab_tar:
    word2ind_tar[word] = ind
    ind+=1
ind2word_tar = {v: k for k, v in word2ind_tar.items()}

import itertools

flat_list_e = list(itertools.chain(*source_tokens))
flat_list_e = list(itertools.chain(*flat_list_e))
vocab_source = sorted(set(flat_list_e))
print(len(flat_list_e), len(vocab_source))
word2ind_source = {}
ind2word_source = {}

ind=1
for word in vocab_source:
    word2ind_source[word] = ind
    ind+=1
ind2word_source = {v: k for k, v in word2ind_source.items()}

vocab_size_target = len(vocab_tar)+1
constant_batch_size = 128

vocab_size_source = len(vocab_source)+1
constant_batch_size = 128

corpus_tr = [[word2ind_tar[word] for word in sample] for sample in target_train_tokens ]#for order in sample]#target->hindi

corpus_tr_test = [[word2ind_tar[word] for word in sample] for sample in hin_test ]#for order in sample]#target->hindi

corpus_sr = [[word2ind_source[word] for word in order] for sample in all_corpus_text_source for order in sample]#source->english

corpus_sr_test = [[word2ind_source[word] for word in order] for sample in all_corpus_text_test_s for order in sample]#source->english

max_length_tr_train = max([len(s) for s in corpus_tr])
print(max_length_tr_train)

z=0
line_numb=[]
for s in hin_test:
  z=z+1
  if len(s)>80:
   line_numb.append(z)
   print(s)
  #  for i in s:
  #    print(ind2word_tar[i])
print(line_numb)

max_length_tr_test = max([len(s) for s in corpus_tr_test])
print(max_length_tr_test)

max_length_tr=max(max_length_tr_train,max_length_tr_test)
print(max_length_tr)



max_length_tr_test = max([len(s) for s in corpus_tr_test])
print(max_length_tr_test)

max_length_sr_train = max([len(s) for s in corpus_sr])
print(max_length_sr_train)

max_length_sr_test = max([len(s) for s in corpus_sr_test])
print(max_length_sr_test)

max_length_sr = max(max_length_sr_train,max_length_sr_test)
print(max_length_sr)

corpus_pad_tr = keras.preprocessing.sequence.pad_sequences(corpus_tr,maxlen=max_length_tr,padding='post')

corpus_pad_tr_test = keras.preprocessing.sequence.pad_sequences(corpus_tr_test,maxlen=max_length_tr,padding='post')

corpus_pad_sr = keras.preprocessing.sequence.pad_sequences(corpus_sr,maxlen=max_length_sr,padding='post')

corpus_pad_sr_test = keras.preprocessing.sequence.pad_sequences(corpus_sr_test,maxlen=max_length_sr,padding='post')

corpus_pad_tr = corpus_pad_tr.reshape(-1,max_length_tr)
corpus_pad_tr.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_tr_test = corpus_pad_tr_test.reshape(-1,max_length_tr)
corpus_pad_tr_test.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_sr = corpus_pad_sr.reshape(-1,3,max_length_sr)
corpus_pad_sr.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

corpus_pad_sr_test = corpus_pad_sr_test.reshape(-1,3,max_length_sr)
corpus_pad_sr_test.shape# corpus of shape(20sententences,normalprepostseq,maxlengh sentence,wordembedding dimension)

y=[0]*10 + [1]*10

print(corpus_pad_tr[0])

y

all_co

# to include zero padding indices to corpus index too

all_corpus_index_order_1D= [order for sample in all_corpus_index_order for order in sample]

all_corpus_index_order_1D_test= [order for sample in all_corpus_index_test_order_s for order in sample]

all_corpus_index_order_1D_pad=[]
for sent in all_corpus_index_order_1D:
    sent_len = len(sent)
    print(sent_len)
    if sent_len < max_length_sr: # logic for padding the index
        new_sent = list(sent)+list(range(sent_len,max_length_sr))
    all_corpus_index_order_1D_pad.append(np.array(new_sent))
all_corpus_index_order_1D_pad = np.array(all_corpus_index_order_1D_pad)

all_corpus_index_order_1D_pad_test=[]
for sent in all_corpus_index_order_1D_test:
    sent_len = len(sent)
    print(sent_len)
    if sent_len < max_length_sr: # logic for padding the index
        new_sent = list(sent)+list(range(sent_len,max_length_sr))
    all_corpus_index_order_1D_pad_test.append(np.array(new_sent))
all_corpus_index_order_1D_pad_test = np.array(all_corpus_index_order_1D_pad_test)

len(all_corpus_index_order_1D_pad_test)

for sent in all_corpus_index_order_1D_pad:
    sent_len = len(sent)
    print(sent_len)

for sent in all_corpus_index_order_1D_pad_test:
    sent_len = len(sent)
    print(sent_len)

all_corpus_index_order_1D_pad_reshape = all_corpus_index_order_1D_pad.reshape(-1,2,all_corpus_index_order_1D_pad.shape[-1])

all_corpus_index_order_1D_pad_reshape_test = all_corpus_index_order_1D_pad_test.reshape(-1,2,all_corpus_index_order_1D_pad_test.shape[-1])

all_corpus_index_order_1D_pad_reshape.shape, corpus_pad_sr.shape

all_corpus_index_order_1D_pad_reshape_test.shape, corpus_pad_sr_test.shape

#for english#source language
input_set_X1,input_set_X2,input_set_X3,input_set_X4,input_set_X5=[],[],[],[],[]
#for sample_ind in range(len(corpus_pad_sr)):
for sample_ind in range(0,1280):
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840):
#for sample_ind in range(0,67):
#for sample_ind in range(5120,6400):
#for sample_ind in range(6400,7680):

#for sample_ind in range(len(X_train)):#for different test data set
    preorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape[sample_ind][0].reshape(corpus_pad_sr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    postorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape[sample_ind][1].reshape(corpus_pad_sr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1.append(corpus_pad_sr[sample_ind][0])
    input_set_X2.append(corpus_pad_sr[sample_ind][1])
    input_set_X3.append(corpus_pad_sr[sample_ind][2])
    input_set_X4.append(preorder_sampled)#index_preorder
    input_set_X5.append(postorder_sampled)#index_postorder
    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

input_set_X1_test,input_set_X2_test,input_set_X3_test,input_set_X4_test,input_set_X5_test=[],[],[],[],[]
for sample_ind in range(128):
#for sample_ind in range(0,67):
#for sample_ind in range(len(X_train)):#for different test data set
    preorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape_test[sample_ind][0].reshape(corpus_pad_sr_test.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    postorder_sampled = np.insert(all_corpus_index_order_1D_pad_reshape_test[sample_ind][1].reshape(corpus_pad_sr_test.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_test.append(corpus_pad_sr_test[sample_ind][0])
    input_set_X2_test.append(corpus_pad_sr_test[sample_ind][1])
    input_set_X3_test.append(corpus_pad_sr_test[sample_ind][2])
    input_set_X4_test.append(preorder_sampled)#index_preorder
    input_set_X5_test.append(postorder_sampled)#index_postorder
    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

#for hindi#target language
input_set_X1_target=[]
#for sample_ind in range(len(corpus_pad_tr)):#len(corpus_pad_tr)
for sample_ind in range(0,1280):#len(corpus_pad_tr)
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840):
#for sample_ind in range(5120,6400):
#for sample_ind in range(6400,7680):
#for sample_ind in range(len(Y_train)):#len(corpus_pad_tr)for different test data set
    # preorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][0].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    # postorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][1].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_target.append(corpus_pad_tr[sample_ind])
    # input_set_X2_h.append(corpus_pad_tr[sample_ind][1])
    # input_set_X3_h.append(corpus_pad_tr[sample_ind][2])
    # input_set_X4_h.append(preorder_sampled_h)#index_preorder
    # input_set_X5_h.append(postorder_sampled_h)#index_postorder

    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

#for hindi#target language
input_set_X1_target_test=[]
#for sample_ind in range(len(corpus_pad_tr_test)):#len(corpus_pad_tr)
for sample_ind in range(0,128):#len(corpus_pad_tr)

#for sample_ind in range(len(Y_train)):#len(corpus_pad_tr)for different test data set
    # preorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][0].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    # postorder_sampled_h = np.insert(all_corpus_index_order_1D_pad_reshape_h[sample_ind][1].reshape(corpus_pad_tr.shape[-1],1), 0, sample_ind%constant_batch_size, axis=1)
    input_set_X1_target_test.append(corpus_pad_tr_test[sample_ind])
    # input_set_X2_h.append(corpus_pad_tr[sample_ind][1])
    # input_set_X3_h.append(corpus_pad_tr[sample_ind][2])
    # input_set_X4_h.append(preorder_sampled_h)#index_preorder
    # input_set_X5_h.append(postorder_sampled_h)#index_postorder

    # input_set_X.append([corpus_pad[sample_ind][0],corpus_pad[sample_ind][1],corpus_pad[sample_ind][2],preorder_sampled,postorder_sampled])
    # input_set_X.append([list(corpus_pad[sample_ind][0]),list(corpus_pad[sample_ind][1]),list(corpus_pad[sample_ind][2]),list(preorder_sampled),list(postorder_sampled)])

pwd

!wget clone https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz

!gunzip cc.en.300.vec.gz

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz

!gunzip cc.hi.300.vec.gz

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.sa.300.vec.gz

!gunzip cc.sa.300.vec.gz

!pip install fasttext

import numpy as np

embeddings_index_en_source = {}
#embeddings_index_hi = {}

k=open("cc.en.300.vec","r")
#l=open("wiki.hi.vec","r")
for line in k:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    if word in vocab_source:
      embeddings_index_en_source[word] = coefs
k.close()

import pickle

a_file = open("drive/MyDrive/data_en_source_red.pkl", "wb")
pickle.dump(embeddings_index_en_source, a_file)
a_file.close()

import numpy as np

embeddings_index_en_target = {}
#embeddings_index_hi = {}

k=open("cc.en.300.vec","r")
#l=open("wiki.hi.vec","r")
for line in k:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    if word in vocab_tar:
      embeddings_index_en_target[word] = coefs
k.close()

import pickle

a_file = open("drive/MyDrive/data_en_target_red.pkl", "wb")
pickle.dump(embeddings_index_en_target, a_file)
a_file.close()

import numpy as np

#embeddings_index_en = {}
embeddings_index_hi = {}

#k=open("wiki.en.vec","r")
l=open("cc.hi.300.vec","r")

for line in l:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    if word in vocab_tar:
          embeddings_index_hi[word] = coefs

l.close()

# print('Found %s word vectors.' % len(embeddings_index_en))
# print('Found %s word vectors.' % len(embeddings_index_hi))

print('Found %s word vectors.' % len(embeddings_index_hi))

import pickle

a_file = open("drive/MyDrive/data_hi_red.pkl", "wb")
pickle.dump(embeddings_index_hi, a_file)
a_file.close()

embeddings_index_sa = {}

z=open("cc.sa.300.vec","r")
for line in z:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    if word in vocab_source:
      embeddings_index_sa[word] = coefs
z.close()

print('Found %s word vectors.' % len(embeddings_index_sa))

import pickle

a_file = open("drive/MyDrive/data_sa_red.pkl", "wb")
pickle.dump(embeddings_index_sa, a_file)
a_file.close()

####run from here#####

import pickle
embeddings_index = {}  # Create an empty dictionary
with open('drive/MyDrive/data_en_source_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file1 to the dictionary

with open('drive/MyDrive/data_en_target_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file1 to the dictionary

with open('drive/MyDrive/data_sa_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file2 to the dictionary

with open('drive/MyDrive/data_hi_red.pkl', 'rb') as f:
    embeddings_index.update(pickle.load(f))   # Update contents of file2 to the dictionary
#print (my_dict_final)

print(len(embeddings_index))

print(embeddings_index['.'])

#not in use
def Merge(dict1, dict2):
    res = {**dict1, **dict2}
    return res
embeddings_index=Merge(embeddings_index_en,embeddings_index_hi)

print(len(embeddings_index))

#not in use

embeddings_index=Merge(embeddings_index,embeddings_index_sa)

pwd



embedding_matrix_source = np.zeros((len(word2ind_source) + 1, 300))
for word, i in word2ind_source.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_source[i] = embedding_vector

word2ind_tar

for word, i in word2ind_tar.items():
    #print(word,i)
    embedding_vector = embeddings_index.get(word)
    print(word,embedding_vector)

ind2word_tar[14575]

embedding_matrix_target = np.zeros((len(word2ind_tar) + 1, 300))
z=0
for word, i in word2ind_tar.items():
    #print(word,i)
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_target[i] = embedding_vector
        #print(i,embedding_matrix_target)
#     else:
#     #     embedding_matrix_target[i]="OOV"
#         z=z+1
#         #print(word,i)
# print(z)

import sys
def sizeof_fmt(num, suffix='B'):
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f %s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f %s%s" % (num, 'Yi', suffix)

for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),
                         key= lambda x: -x[1])[:10]:
  print("{:>30}: {:>8}".format(name, sizeof_fmt(size)))

embedding_matrix_target: 10.9 MiB
       embedding_matrix_source: 10.0 MiB
 all_corpus_index_order_1D_pad:  1.3 MiB
              embeddings_index: 576.1 KiB
                   flat_list_e: 297.4 KiB
                  word2ind_tar: 144.1 KiB
                  ind2word_tar: 144.1 KiB
               word2ind_source: 144.1 KiB
               ind2word_source: 144.1 KiB
                     vocab_tar: 128.2 KiB

#from keras.utils import to_categorical
from tensorflow.keras.utils import to_categorical


X1_h_new_list=to_categorical(input_set_X1_target, num_classes = vocab_size_target, dtype ="int32")

import sys
def sizeof_fmt(num, suffix='B'):
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f %s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f %s%s" % (num, 'Yi', suffix)

for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),
                         key= lambda x: -x[1])[:10]:
  print("{:>30}: {:>8}".format(name, sizeof_fmt(size)))

# Functional API
from tensorflow.keras.layers import Embedding, LSTM, Dense, Concatenate, Lambda,Bidirectional,Attention
from keras.layers import  RepeatVector

from tensorflow.keras import Model, Input
from tensorflow.keras.utils import plot_model
from keras.layers.wrappers import TimeDistributed
from tensorflow.keras import layers
EMBEDDING_DIM=300
inp_layer_normal=Input(shape=(None,), batch_size=constant_batch_size, name="normal_input")
inp_layer_pre=Input(shape=(None,), batch_size=constant_batch_size, name="preorder_input")
inp_layer_post=Input(shape=(None,), batch_size=constant_batch_size, name="postorder_input")
inp_index_pre=Input(shape=(None,2), batch_size=constant_batch_size, dtype=tf.int32, name="preorder_index")
inp_index_post=Input(shape=(None,2), batch_size=constant_batch_size, dtype=tf.int32, name="postorder_index")


mid_layer_normal = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_normal)
mid_layer_pre = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_pre)
mid_layer_post = Embedding(len(word2ind_source) + 1,EMBEDDING_DIM,weights=[embedding_matrix_source],input_length=max_length_sr,trainable=True)#(inp_layer_post)

embd_normal=mid_layer_normal(inp_layer_normal)
embd_pre=mid_layer_pre(inp_layer_pre)
embd_post=mid_layer_post(inp_layer_post)

# mid_layer_normal=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_normal) # not sharing the embedding layer
# mid_layer_pre=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_pre)
# mid_layer_post=Embedding(input_dim=vocab_size_source,output_dim=50)(inp_layer_post)
#encoder_lstm1 = LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)


mid_layer_normal=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_normal)
mid_layer_pre=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_pre)
mid_layer_post=LSTM(units=EMBEDDING_DIM,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(embd_post)

#mid_layer_normal=Bidirectional(LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True))(mid_layer_normal)
#mid_layer_normal=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_normal)
#mid_layer_normal=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_normal)
#mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
#mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
#mid_layer_pre=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_pre)
#mid_layer_post=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_post)
#mid_layer_post=LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True)(mid_layer_post)
#mid_layer_post=LSTM(units=64,dropout=0.2,recuindrrent_dropout=0.2, return_sequences=True)(mid_layer_post)

# mid_layer_normal = Lambda(lambda x: tf.gather_nd(x[0],x[1]))([mid_layer_normal,inp_index_order])
mid_layer_pre = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_preorder")([mid_layer_pre,inp_index_pre])
mid_layer_post = Lambda(lambda x: tf.gather_nd(x[0],x[1]), name="reindex_postorder")([mid_layer_post,inp_index_post])

concat_layer = Concatenate(axis=2)([mid_layer_normal,mid_layer_pre,mid_layer_post])


#concat_layer = Merge([mid_layer_normal,mid_layer_pre,mid_layer_post])

# encoder_inputs = Input(shape=(None,))
#enc_emb =  Embedding(input_dim=vocab_size_source,output_dim=50)(concat_layer)
encoder_lstm = LSTM(units=EMBEDDING_DIM,return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(concat_layer)
# # We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

#output, state_h, state_c =Concatenate(axis=2)([mid_layer_normal,mid_layer_pre,mid_layer_post])
print(concat_layer.shape)
print(concat_layer)

# encoder_inputs = Input(shape=(None, num_encoder_tokens))
# encoder = LSTM(units=64, return_state=True)
# enc_output, state_h, state_c = LSTM(units=64, return_state=True, name="encoder")(concat_layer)
# # encoder_outputs, state_h, state_c = concat_layer
# # # We discard `encoder_outputs` and only keep the states.
# encoder_states = [state_h, state_c]
#  #Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,), batch_size=constant_batch_size, name="decoder input")
dec_emb_layer= Embedding(len(word2ind_tar) + 1,EMBEDDING_DIM,weights=[embedding_matrix_target],input_length=max_length_tr,trainable=True)

#dec_emb_layer = Embedding(input_dim=vocab_size_target,output_dim=50)
# dec_emb = dec_emb_layer(decoder_inputs)

# # We set up our decoder to return full output sequences,
# # and to return internal states as well. We don't use the
# # return states in the training model, but we will use them in inference.
decoder_embedding=dec_emb_layer(decoder_inputs) # not sharing the embedding layer
decoder_lstm = LSTM(EMBEDDING_DIM, return_sequences=True, return_state=True)

#decoder_lstm = LSTM(units=64,dropout=0.2,recurrent_dropout=0.2, return_sequences=True,return_state=True)#(decoder_embedding,initial_state=encoder_states)
#decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
# attn_layer = Attention(name='attention_layer')
# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])

# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

#dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')
# decoder_pred = dense_time(decoder_concat_input)



decoder_dense = Dense(units=vocab_size_target, activation='softmax', name='softmax_layer')
dense_time = tf.keras.layers.TimeDistributed(decoder_dense, name='time_distributed_layer')

decoder_outputs = dense_time(decoder_outputs)

# # Define the model that will turn
# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,decoder_inputs], decoder_outputs)

# # Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy')

#output = TimeDistributed(Dense(units=vocab_size_target, activation='sigmoid'))(decoder_outputs)
#Dense(units=1, activation='sigmoid')
# # # output_layer = layers.Dense(10)(decoder_output)

# # #output_layer=(Dense(units=1,activation='softmax'))(concat_layer) # To be replaced with decoder
# # #model = keras.Model([encoder_input, decoder_input], output)

# #model = Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post], output_layer)
# # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
#model = keras.Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post,decoder_inputs], output)
#model.compile(optimizer='adam', loss='binary_crossentropy')

#model.summary()
print(model.summary())
plot_model(model, to_file='/content/drive/My Drive/model.png')

def shift(lst,vocab_size_h):
    new_list = []
    for sent in lst:
      sent_list=[]
      for p in range(len(sent)-1):

        sent_list.insert(p, sent[p+1])
      z=np.zeros((vocab_size_h))
      z[0]=1
      sent_list.append(z)
      new_list.append(sent_list)
      #print("length",len(sent_list))
      #print(sent[p+1].shape)



    return new_list

out_shift=shift(X1_h_new_list,vocab_size_target)

from keras.models import  load_model

filepath = "drive/MyDrive/dep_sa_hi_endep/model.h5"

model = load_model(filepath)

from numpy.testing import assert_allclose
from keras.models import  load_model
#from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import ModelCheckpoint,EarlyStopping

es = EarlyStopping(monitor='val_loss', mode='min', patience=5)
filepath = "drive/MyDrive/dep_sa_hi_endep/model.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint,es]
#callbacks_list = [checkpoint]

from numpy.testing import assert_allclose
from keras.models import  load_model
#from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import ModelCheckpoint,EarlyStopping

# es = EarlyStopping(monitor='val_loss', mode='min', patience=50)
# filepath = "drive/MyDrive/10000epocs/model.h5"
# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
# callbacks_list = [checkpoint,es]
#history=model.fit([np.array(X1_train),np.array(X2_train),np.array(X3_train),np.array(X4_train),np.array(X5_train),np.array(Y_train)],np.array(out_shift_t),batch_size=constant_batch_size,epochs=10, verbose=1,callbacks=callbacks_list,validation_data=([np.array(X1_val),np.array(X2_val),np.array(X3_val),np.array(X4_val),np.array(X5_val),np.array(Y_val)],),validation_batch_size=constant_batch_size)

history=model.fit([np.array(input_set_X1),np.array(input_set_X2),np.array(input_set_X3),np.array(input_set_X4),np.array(input_set_X5),np.array(input_set_X1_target)],np.array(out_shift),batch_size=constant_batch_size,epochs=20, verbose=1,callbacks=callbacks_list,validation_split=0.2)
#!mkdir -p saved_model
#model.save('saved_model/my_model')
#checkpoint = ModelCheckpoint(filepath='/content/check_pt/model.h5', monitor='val_loss',verbose=1, save_best_only=True, mode='min')

filepath = "drive/MyDrive/dep_sa_hi_endep/model.h5"

model = load_model(filepath)

# encoder_inputs1,encoder_inputs2,encoder_inputs3,encoder_inputs4,encoder_inputs5 = model.input[0],model.input[1],model.input[2],model.input[3],model.input[4]  # input_1
# encoder_outputs, state_h_enc, state_c_enc = model.layers[16].output  # lstm_1
# encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model([inp_layer_normal,inp_layer_pre,inp_layer_post,inp_index_pre,inp_index_post], encoder_states)

# Decoder setup
# Below tensors will hold the states of the previous time step
latent_dim=300
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence

# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary

# Final decoder model
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2)

print()

plot_model(decoder_model, to_file='/content/drive/My Drive/decoder_model.png')

pwd

def decode_sequence(input_seq):
    indices=[]
    target=[]
    newtarget=[]
    #states_value = encoder_model.predict(input_seq)
    #states_value=np.reshape(states_value, (32,2,1,300))
    #target_seq = np.zeros((32,1))
    # z=[]
    # for p in range(2):
    #   z.append([])
    #   for q in range(32):
    #     z[p].append(states_value[p][q])
    # print(len(z))
    # print(len(z[0]))
    # print(np.shape(z[1]))
    #target_seq[0, 0] = word2ind_tar['sos']

    for j in range(constant_batch_size):
      target.append([])
      #print(j)
      stop_condition = False
      states_value = encoder_model.predict(input_seq,batch_size=constant_batch_size)

      #print(np.shape(states_value))
      target_seq = np.zeros((constant_batch_size,1))
      target_seq[j, 0] = word2ind_tar['sos']
      # target_seq1=target_seq[j, 0]
      # print(target_seq1.shape)
      k=0

      while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value,batch_size=constant_batch_size)

        #print(output_tokens.shape)

        #for i in range(len(output_tokens[-2])):
        #integers = np.argmax(output_tokens[j])
        #print(output_tokens)
        integers = np.argmax(output_tokens[j])

        #print(integers)
        #stop_condition = True

        #i=i+1
        if integers==0:
          word="PADDING"
        else:
          integers=integers
          word = ind2word_tar[integers]
          #print(word)
        target[j].append(word)
        #print(target)
        #stop_condition = True
        if (word == 'eos' or len(target[j]) > 15):
            stop_condition = True

        if word is None:
 	 	        break
        #  # Update the target sequence (of length 1).
        target_seq = np.zeros((constant_batch_size,1))
        target_seq[j, 0] = integers
        #if target_train[j][k] != "eos":
        # if target_tokens_test[j][k] != "eos":
        #   k=k+1
        #print(k)
        #target_seq[j, 0] = word2ind_tar[target_train[j][k]]
        # target_seq[j, 0] = word2ind_tar[target_tokens_test[j][k]]
        # Update states
        states_value = [h, c]
      #print(target)
    #print(target)

    #target[j].append(word)

   # print(len(target[0]))
    for i in range(len(target)):
      newtarget.append([])
      for ele in target[i]:
        b=ele
        if b=='PADDING':
         del(b)
        else:
         newtarget[i].append(b)
# print(newtarget)



    return newtarget#decode_sentence
    #return target#decode_sentence

decoded_sentence1=decode_sequence([np.array(input_set_X1[0:128]),np.array(input_set_X2[0:128]),np.array(input_set_X3[0:128]),np.array(input_set_X4[0:128]),np.array(input_set_X5[0:128])])

print(decoded_sentence1)

decoded_sentence=decode_sequence([np.array(input_set_X1_test),np.array(input_set_X2_test),np.array(input_set_X3_test),np.array(input_set_X4_test),np.array(input_set_X5_test)])

print(decoded_sentence)

#ot training history
from matplotlib import pyplot

pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

print(history.history['loss'])
print(history.history['val_loss'])

output=model.predict([np.array(input_set_X1_test),np.array(input_set_X2_test),np.array(input_set_X3_test),np.array(input_set_X4_test),np.array(input_set_X5_test),np.array(input_set_X1_target_test)],batch_size=constant_batch_size,verbose=1)

output1=model.predict([np.array(input_set_X1),np.array(input_set_X2),np.array(input_set_X3),np.array(input_set_X4),np.array(input_set_X5),np.array(input_set_X1_target)],batch_size=constant_batch_size,verbose=1)

import numpy as np

from numpy import argmax
indices=[]
#target = list()
target1=[]
for j in range(len(output1)):
  target1.append([])
  for i in range(len(output1[-2])):
    integers = np.argmax(output1[j][i])
    # if integers==vocab_size_h-1:
    #     integers=integers
    # else:
    #     integers=integers+1
    if integers==0:
       #integers=integers+1
        word="PADDING"
    else:
       integers=integers
       word = ind2word_tar[integers]
    if word is None:
 	 	     break
    #target[j].append(word)
    target1[j].append(word)

    #a.replace('"', '')
    #target[j].append(word)

    #target[j]
print(target1)

import numpy as np

from numpy import argmax
indices=[]
#target = list()
target=[]
for j in range(len(output)):
  target.append([])
  for i in range(len(output[-2])):
    integers = np.argmax(output[j][i])
    # if integers==vocab_size_h-1:
    #     integers=integers
    # else:
    #     integers=integers+1
    if integers==0:
       #integers=integers+1
        word="PADDING"
    else:
       integers=integers
       word = ind2word_tar[integers]
    if word is None:
 	 	     break
    #target[j].append(word)
    target[j].append(word)

    #a.replace('"', '')
    #target[j].append(word)

    #target[j]
print(target)

print(len(target))

#a=target[5]
#print(a)
newtarget1=[]
#z=max_length_tr-1
for i in range(len(target1)):
  newtarget1.append([])
  for ele in target1[i]:
    b=ele
    if b=='PADDING':
     del(b)
    else:
      newtarget1[i].append(b)
print(newtarget1)
#   if ele=='0,0':
#       a.remove(ele)
#   else:
#     ele=ele
# #      #z=z-1
# print(a)



#a=target[5]
#print(a)
newtarget=[]
#z=max_length_tr-1
for i in range(len(target)):
  newtarget.append([])
  for ele in target[i]:
    b=ele
    if b=='PADDING':
     del(b)
    else:
      newtarget[i].append(b)
print(newtarget)
#   if ele=='0,0':
#       a.remove(ele)
#   else:
#     ele=ele
# #      #z=z-1
# print(a)

!pip install -U nltk

from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
#from nltk.translate.meteor_score import meteor_score
import nltk
import nltk.translate.gleu_score as gleu
#import nltk.translate.meteor_score.meteor_score

smoother = SmoothingFunction()
def evaluate_model(eng,hin,target):
      actual, predicted = list(),list()
      bb1=0
      bb2=0
      bb3=0
      bb4=0
      gleu_list=[]
      #gleu1=0
      c=0

      for i in range(128):
        raw_target, raw_src, tar  = hin[i],eng[i],target[i]
        tar=tar[0:-1]
        raw_target=raw_target[1:-1]
        raw_src=raw_src[1:-1]
        #tar=[tar]
        #raw_target=[raw_target]
        #raw_src=raw_src[1:-1]
        if i < 128:
		        print('src=%s, target=%s, pred=%s' % (raw_src, raw_target, tar))


  #     actual.append(raw_target)
  #     predicted.append(tar)
  # print(actual)
  # print(predicted)
        # score_ref_a = gleu.sentence_gleu([raw_target], tar)
        # print("Hyp and ref_a are the same: {}".format(score_ref_a))
        # wer_score(raw_target, tar, print_matrix=True)
        try:
          bb1+=(sentence_bleu([raw_target],tar,weights=(1,0,0,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(1,0,0,0), smoothing_function=smoother.method7))/2
          print(bb1)
          #meteor+=nltk.translate.meteor_score.meteor_score([raw_target], "tar")
          bb2+=(sentence_bleu([raw_target],tar,weights=(0,1,0,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,1,0,0), smoothing_function=smoother.method7))/2
          print(bb2)
          bb3+=(sentence_bleu([raw_target],tar,weights=(0,0,1,0), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,0,1,0), smoothing_function=smoother.method7))/2
          print(bb3)
          bb4+=(sentence_bleu([raw_target],tar,weights=(0,0,0,1), smoothing_function=smoother.method2)+sentence_bleu(raw_target,tar,weights=(0,0,0,1), smoothing_function=smoother.method7))/2
          print(bb4)
          gleu1=gleu.sentence_gleu([raw_target], tar,min_len=1, max_len=1)
          print(gleu1)
          gleu_list.append(gleu1)
          #z=nltk.translate.chrf_score.chrf_precision_recall_fscore_support([raw_target], tar, 1, beta=3.0, epsilon=1e-16)
          #print(z)
        except ZeroDivisionError:
          pass
        # c+=1
        # if c%4==0:
          # print(c)
          # print(raw_src)
          # print(raw_target)
          # print(tar)
      bleu1=bb1*100/128
      bleu2=bb2*100/128
      bleu3=bb3*100/128
      bleu4=bb4*100/128
      gleu_avg=sum(gleu_list)/len(gleu_list)
      return bleu1,bleu2,bleu3,bleu4,gleu_avg

from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu

references = ['सभी', 'वचन', 'निभायेंगे', ':', 'मंत्री', 'श्री', 'शर्मा']
candidates = ['सभी', 'वचन', 'निभायेंगे', ':', 'मंत्री', 'श्री', 'शर्मा']
score = corpus_bleu(references, candidates)
print(score)



###day 11/06/21###with 1280 sentences for with san_eng new dataset san dep removed only eng dep

source_train=san_etokens[0:128]

target_train=eng_stokens[0:128]

source_tokens_test=san_test[0:128]

target_tokens_test=hin_test[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######

###day 11/06/21###with 1280 sentences for with san_eng new dataset(5120-6400) san dep removed

source_train=eng_htokens[120:248]

target_train=hin_etokens[120:248]

source_tokens_test=san_test[0:128]

target_tokens_test=hin_test[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######



###day 11/06/21###with 1280 sentences for with san_eng new dataset(6400-7680) san dep removed

source_train=eng_htokens[1400:1528]

target_train=hin_etokens[1400:1528]

source_tokens_test=san_test[0:128]

target_tokens_test=hin_test[0:128]

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

evaluate_model(source_train,target_train,decoded_sentence1)

#####end ######

0,1280):#len(corpus_pad_tr)
#for sample_ind in range(1280,2560):#len(corpus_pad_tr)
#for sample_ind in range(2560,3840)

target_train=target_tokens[0:1280]

target_train=target_tokens[2560:3840]

target_train=target_tokens[1280:2560]

source_train=source_tokens[0:1280]

source_train=source_tokens[1280:2560]

source_train=source_tokens[2560:3840]

source_tokens_test=san_test

target_tokens_test=hin_test

print(target_tokens[100])

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_train,target_train,newtarget1)

evaluate_model(source_tokens_test,target_tokens_test,newtarget)

b=evaluate_model(source_train,target_train,decoded_sentence1)
print("bleu_score",b)

b=evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)
print("bleu_score",b)

evaluate_model(source_tokens_test,target_tokens_test,decoded_sentence)

print("bleu_score",b)

"""Rough work"""

a=gauss_2d(0,1)

print(a)

inp = [[[10,0,0.1], [20,1,1.1], [30,2,2.1], [40,3,3.1], [50,4,4.1]],
       [[100,0.5,0.6], [200,1.5,1.6], [300,2.5,2.6], [400,3.5,3.6], [500,4.5,4.6]]]

tf.gather_nd(np.array(inp), [[[0, 1],[0, 2],[0,3],[0,0],[0,4]],
                             [[1, 1],[1, 2],[1,3],[1,4],[1,0]]])